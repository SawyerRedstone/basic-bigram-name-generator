{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52533478-9b0b-41b8-8b6a-e3cf89d940a7",
   "metadata": {},
   "source": [
    "# Generating Names with Bigrams\n",
    "<!-- Basic Bigram Generator -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f19b2db-e84f-44df-92a5-945800aff081",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1) [Project Overview](#overview)\n",
    "2) [Preparing the Data](#data)\n",
    "3) [Building the Bigram Model](#model)\n",
    "4) [Training the Model](#training)\n",
    "5) [Generating Names](#generate)\n",
    "6) [Next Steps](#next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2fae51-6189-4860-8e21-5a18aa9c0cea",
   "metadata": {},
   "source": [
    "<a id=\"overview\"></a>\n",
    "## Project Overview \n",
    "The goal of this project is to create a simple language model that can generate names resembling real ones from its training data.\n",
    "\n",
    "We'll use a neural network trained on character-level bigrams -- that is, pairs of consecutive letters. When the model is given a letter as input, it should predict the probability of each possible next letter. We can then use these probabilities to pick the next letter in a name, then feed that new letter back into the model to continue generating the name, one letter at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced6390-bee6-4aaf-adc4-1dab1eaa6e5f",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "## Preparing the Data\n",
    "We'll start by reading in a list of names we will use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea8ac9df-0b96-4287-9591-4a2de6feba44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the first three names: ['emma', 'olivia', 'ava']\n"
     ]
    }
   ],
   "source": [
    "# Create a list of each name in names.txt\n",
    "names = open(\"names.txt\", \"r\").read().splitlines()\n",
    "print(f\"Displaying the first three names: {names[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2689af61-4b56-4456-a36e-4062f2b94d39",
   "metadata": {},
   "source": [
    "Now that we have our data, let's split it into bigrams so the model can learn common letter patterns in names. We also want it to learn how each name starts and ends, so let's indicate the start and end of a name with a \".\". Below shows an example of the bigrams in the name Emma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a2761a5-07ca-4343-bf02-5b2862fadba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams for emma: [('.', 'e'), ('e', 'm'), ('m', 'm'), ('m', 'a'), ('a', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Break \"emma\" into bigrams, including \".\" for start and end.\n",
    "print(f\"Bigrams for emma: {list(zip('.emma', 'emma.'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532cd13c-5f08-43d5-abf7-e809ae7bd592",
   "metadata": {},
   "source": [
    "To use these bigrams in our neural network, we'll organize them into two tensors: X (inputs) and y (labels).\n",
    "- X will contain the first letter of each bigram. These are the inputs we'll feed into the model.\n",
    "- y will contain the second letter of each bigram, i.e., the letter that follows each input letter in X. These are the labels or \"targets\" the model should learn to predict.\n",
    "\n",
    "To start off, let's import Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72dcc032-dbd1-4517-871e-9d48ab9549e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch    # Tensors and backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a42fb6-f8c3-4e48-b3e5-3af0feed7bea",
   "metadata": {},
   "source": [
    "Since PyTorch tensors can't contain character elements, let's assign each letter to an integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67fcb3e7-03f7-46f8-b54b-981b755310f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# For each letter + \".\", add them to dict with an integer value.\n",
    "stoi = {char:int_val for (int_val, char) in enumerate(\".\" + string.ascii_lowercase)}\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ee9ab-b714-4d56-9f94-95f1caa38847",
   "metadata": {},
   "source": [
    "We'll also want a way to go backwards, turning numbers back into letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec968468-135b-48dd-a618-f2ee5b75add6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "itos = {num: letter for letter, num in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c273ea97-83a4-4cd3-8753-6e54802ec317",
   "metadata": {},
   "source": [
    "Now we are ready to split all the names into bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "353fc34a-6aac-4f6b-b64f-c4d90cac8006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training data has 228146 examples.\n",
      "The first 10 values in X are [0, 5, 13, 13, 1, 0, 15, 12, 9, 22].\n",
      "The first 10 values in y are [5, 13, 13, 1, 0, 15, 12, 9, 22, 9].\n"
     ]
    }
   ],
   "source": [
    "X = [] # First letters in bigrams\n",
    "y = [] # X's bigram pairs.\n",
    "\n",
    "for name in names:\n",
    "    for char1, char2 in zip(\".\" + name, name + \".\"):\n",
    "        # Convert chars to ints so we can later add to tensors.\n",
    "        X.append(stoi[char1])\n",
    "        y.append(stoi[char2])\n",
    "\n",
    "print(f\"Our training data has {len(X)} examples.\")\n",
    "print(f\"The first 10 values in X are {X[:10]}.\")\n",
    "print(f\"The first 10 values in y are {y[:10]}.\")\n",
    "\n",
    "# Convert list to a tensor.\n",
    "X = torch.tensor(X)\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc1c15a-4e0f-48b5-9805-3831086ff727",
   "metadata": {},
   "source": [
    "Before we can use these values in a neural network, we'll need to alter them a bit more. See, neural networks contain weights which get multiplied by each input. Right now our inputs are just numbers from 0 to 26. It wouldn't be very helpful to do math with these inputs, since we want each letter to be treated equally. \"z\" shouldn't have a different multiplier than \"a\", for example.\n",
    "\n",
    "To make sure each input gets treated equally, we can use one-hot encoding. With one-hot encoding, each letter gets represented by an array of length 27 (one for each possible letter including \".\"). This array contains all 0s apart from a single 1 at the index signifying the chosen letter. To show this more visually, let's explore the one-hot encoded version of the letter \"c\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "808bffe7-caa8-4575-88a5-8508992ec230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c's index is 3, so its one-hot encoding looks like tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Get number used to represent c. This will be the index of the 1 after one-hot encoding.\n",
    "c_index = stoi[\"c\"]\n",
    "\n",
    "# Create the one-hot encoded representation of the letter \"c\".\n",
    "c_enc = torch.zeros(27)\n",
    "c_enc[c_index] = 1\n",
    "\n",
    "print(f\"c's index is {c_index}, so its one-hot encoding looks like {c_enc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18171a9-9ec0-4875-a966-5d376093342e",
   "metadata": {},
   "source": [
    "Let's apply this encoding to all the letters in our input data. Rather than encoding them all manually like above, we'll use PyTorch's one_hot() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45a02bce-6d1e-4b42-8deb-811882958e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# One-hot encode X to turn each letter into an array of length 27 (one index for each possible letter including \".\".)\n",
    "X_enc = F.one_hot(X, num_classes = 27)\n",
    "# View encodings for first 5 letters.\n",
    "X_enc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85bfee4-57eb-4438-ad74-bffa495dac25",
   "metadata": {},
   "source": [
    "As you can see, the values here are all integers. To later multiply them by the float weights in our neural network, we'll need to convert those values into floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "088e945f-df59-4662-9795-07e836c53985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want the neural net to produce floats, so the inputs must be floats as well.\n",
    "X_enc = X_enc.float()\n",
    "# The first letter should now be encoded with floats.\n",
    "X_enc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e5fdd6-8340-4902-9662-c7dbfc5455f4",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "## Building the Bigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6215d93b-154e-49a3-ab4a-1867125439f8",
   "metadata": {},
   "source": [
    "For this project, we'll use a very simple neural network with only a single layer of neurons. \n",
    "\n",
    "Before we jump into building this layer, it's important to understand how a single neuron works.\n",
    "\n",
    "Since each letter in X now has a length of 27 (due to the encoding), this one neuron will need 27 weights so each element in the inputs can be multiplied by a unique weight. The weights will start off as random numbers from a normal distribution. (After training the model, these numbers should become more meaningful.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7fb0bfe-1f20-448c-910d-380bd8c428c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a single neuron with a 27 weights:\n",
      "tensor([[ 1.9269],\n",
      "        [ 1.4873],\n",
      "        [ 0.9007],\n",
      "        [-2.1055],\n",
      "        [ 0.6784],\n",
      "        [-1.2345],\n",
      "        [-0.0431],\n",
      "        [-1.6047],\n",
      "        [-0.7521],\n",
      "        [ 1.6487],\n",
      "        [-0.3925],\n",
      "        [ 0.2415],\n",
      "        [-1.1109],\n",
      "        [ 0.0915],\n",
      "        [-2.3169],\n",
      "        [-0.2168],\n",
      "        [-1.3847],\n",
      "        [-0.8712],\n",
      "        [-0.2234],\n",
      "        [-0.6216],\n",
      "        [-0.5920],\n",
      "        [-0.0631],\n",
      "        [-0.8286],\n",
      "        [ 0.3309],\n",
      "        [-1.5576],\n",
      "        [ 0.9956],\n",
      "        [-0.8798]])\n"
     ]
    }
   ],
   "source": [
    "# Weights start off random, so let's use a generator.\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "\n",
    "# A single neuron with one weight for each element in our input.\n",
    "W = torch.randn((27, 1), generator=gen)\n",
    "print(\"Here is a single neuron with a 27 weights:\")\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b5d0b-e7b8-44ea-920f-34b382eca936",
   "metadata": {},
   "source": [
    "Now we can use this neuron by multiplying our inputs with it.\n",
    "\n",
    "\n",
    "When the input is a one-hot encoded vector, the dot product multiplies all weights by 0 except for the one weight at the position where the input has a 1. Each of the resulting multiplications are then added together, but since most of the values are 0, they end up ignored by the result. Thus, it is as if each one-hot encoded input simply selects the corresponding weight from the neuron and ignores the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9a6fe85-6afa-4ee7-9c92-623e1cde8602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_enc has a 1 at index 3, so the dot product should select the neuron's weight at index 3.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-2.1055])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"c_enc has a 1 at index 3, so the dot product should select the neuron's weight at index 3.\")\n",
    "c_enc @ W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660294e0-4975-4045-98b4-a9464b6a097a",
   "metadata": {},
   "source": [
    "As described earlier, we want our model to take a letter as input and output the probability of each possible next letter. Since there are 27 possible next letters, the model should produce a vector of 27 probabilities, one for each letter.\n",
    "\n",
    "We've seen that a single neuron produces one output. Thus, to produce 27 outputs, we'll need a layer of 27 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a5caf1e-96f7-4813-b82a-a86550bd4311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting generator so cell always gives same numbers instead of generating the next numbers.\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "\n",
    "# Each column in the weights matrix represents another neuron in the layer.\n",
    "W = torch.randn((27, 27), generator = gen, requires_grad=True)    # `requires_grad` will be explained later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01f61b3-0c77-4414-8478-447c26b5bf76",
   "metadata": {},
   "source": [
    "Now W (our weights) is a 27x27 matrix of random values from a normal distribution. Each column represents a single neuron in the layer. \n",
    "\n",
    "Before we explored how one-hot encoding selected a single value from the neuron. That's still true, but now that there are 27 neurons, it will select the value in that same row for each of the neurons. So for example, an input of c (index 3) dot producted with the layer would result in a vector of all the weights at the row with index 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f033286-894f-4c58-a3a2-3498b0dc1500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3387, -1.3407, -0.5854,  0.5362,  0.5246,  1.1412,  0.0516,  0.7440,\n",
       "        -0.4816, -1.0495,  0.6039, -1.7223, -0.8278,  1.3347,  0.4835, -2.5095,\n",
       "         0.4880,  0.7846,  0.0286,  0.6408,  0.5832,  1.0669, -0.4502, -0.1853,\n",
       "         0.7528,  0.4048,  0.1785], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the third weight from each neuron.\n",
    "c_output = c_enc @ W\n",
    "c_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da32450-2be9-4486-a1d5-e6424c5e64ca",
   "metadata": {},
   "source": [
    "Now we have a model that can take a letter as input and produce 27 outputs, one for each possible next letter. \n",
    "\n",
    "You may notice that the numbers in our output don't look very much like probabilities just yet. At this stage, the outputs are just raw scores (also called \"logits\"), not probabilities. When we use this model during training and sampling, we'll need to normalize these outputs to turn them into probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "911ffc48-a3c9-4915-94b6-fb39cca04b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our untrained model's current raw scores for each letter following 'c':\n",
      ".: -0.3387\n",
      "a: -1.3407\n",
      "b: -0.5854\n",
      "c: 0.5362\n",
      "d: 0.5246\n",
      "e: 1.1412\n",
      "f: 0.0516\n",
      "g: 0.7440\n",
      "h: -0.4816\n",
      "i: -1.0495\n",
      "j: 0.6039\n",
      "k: -1.7223\n",
      "l: -0.8278\n",
      "m: 1.3347\n",
      "n: 0.4835\n",
      "o: -2.5095\n",
      "p: 0.4880\n",
      "q: 0.7846\n",
      "r: 0.0286\n",
      "s: 0.6408\n",
      "t: 0.5832\n",
      "u: 1.0669\n",
      "v: -0.4502\n",
      "w: -0.1853\n",
      "x: 0.7528\n",
      "y: 0.4048\n",
      "z: 0.1785\n"
     ]
    }
   ],
   "source": [
    "print(\"Our untrained model's current raw scores for each letter following 'c':\")\n",
    "for letter, logit in zip(stoi.keys(), c_output):\n",
    "    print(f\"{letter}: {logit:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8318672-2de8-487e-90eb-43d3882c6c18",
   "metadata": {},
   "source": [
    "Now that we understand what happens when we use a single letter as input to our neural network, let's look at what happens with the full dataset. Our input data, stored in `X_enc`, contains far more than just one letter. When we pass these inputs through the network, each one-hot encoded row in `X_enc` selects a corresponding row from the weights matrix to produce its logits. These logits are then stacked together, resulting in an output matrix where each row contains the logits for one input letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97ce7428-347b-4a32-91c4-dd16c0ff89bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873,  0.9007,  ...,  1.2791,  1.2964,  0.6105],\n",
       "        [ 0.5760,  1.1415,  0.0186,  ...,  0.8123, -1.9006,  0.2286],\n",
       "        [-0.3303, -0.7939,  0.3752,  ...,  0.6854, -0.1397, -1.1808],\n",
       "        ...,\n",
       "        [-1.1798, -0.5297,  0.9625,  ..., -0.7068, -1.2520,  3.0250],\n",
       "        [ 1.3463,  0.8556,  0.3220,  ..., -1.4740, -0.3502,  0.4590],\n",
       "        [ 0.4557,  0.2503, -1.3611,  ...,  2.1296, -1.5181,  0.1387]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get logits for each letter in X.\n",
    "logits = X_enc @ W\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4931c7e9-68c8-4385-9b81-3a99a4e32a89",
   "metadata": {},
   "source": [
    "### Turning Logits to Probabilities with Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3682206-2be3-483c-a24d-3220fadb7131",
   "metadata": {},
   "source": [
    "Earlier, we mentioned that we'd like to turn these logits into probabilities. We can do this using the softmax function, which works like so:\n",
    "\n",
    "First, we exponentiate each of the logits. This turns all values positive while keeping their relative order. All negative numbers will turn into a value between 0 and 1, and all positive values will end up as some value larger than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ae9a578-7c2c-4b6a-aea2-4a54de73c760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.8683,  4.4251,  2.4614,  ...,  3.5935,  3.6562,  1.8413],\n",
       "        [ 1.7789,  3.1315,  1.0187,  ...,  2.2532,  0.1495,  1.2568],\n",
       "        [ 0.7187,  0.4521,  1.4553,  ...,  1.9845,  0.8696,  0.3070],\n",
       "        ...,\n",
       "        [ 0.3074,  0.5888,  2.6183,  ...,  0.4932,  0.2859, 20.5950],\n",
       "        [ 3.8430,  2.3528,  1.3799,  ...,  0.2290,  0.7045,  1.5825],\n",
       "        [ 1.5773,  1.2844,  0.2564,  ...,  8.4117,  0.2191,  1.1488]],\n",
       "       grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change values to all be positive.\n",
    "pos_values = logits.exp()\n",
    "pos_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d970135e-e72c-460e-96e4-21829dafe893",
   "metadata": {},
   "source": [
    "Then we turn this into a probability by summing up each row and setting each logit in that row equal to their fraction of that sum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05b6ca6b-004c-4c54-837f-ed9b72febf9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1230, 0.0793, 0.0441,  ..., 0.0644, 0.0655, 0.0330],\n",
       "        [0.0396, 0.0698, 0.0227,  ..., 0.0502, 0.0033, 0.0280],\n",
       "        [0.0123, 0.0078, 0.0250,  ..., 0.0340, 0.0149, 0.0053],\n",
       "        ...,\n",
       "        [0.0044, 0.0085, 0.0378,  ..., 0.0071, 0.0041, 0.2970],\n",
       "        [0.0843, 0.0516, 0.0303,  ..., 0.0050, 0.0154, 0.0347],\n",
       "        [0.0435, 0.0354, 0.0071,  ..., 0.2319, 0.0060, 0.0317]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum the column vectors to calculate the sum of each row, then divide each element in the row by the result to get their probabilities.\n",
    "probs = pos_values / pos_values.sum(dim=1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d5ac5b-8bb0-48d8-ba2c-b84b8b7629ff",
   "metadata": {},
   "source": [
    "If we did softmax correctly, each row should now add up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6a9b413-5d42-4b4b-a2ed-ffa742595607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        ...,\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm that each row now sums to 100%.\n",
    "probs.sum(dim=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedc9018-019a-42ea-b527-1c075cbfc214",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"training\"></a>\n",
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1c9284-383b-4343-90e7-ac7c51b7d462",
   "metadata": {},
   "source": [
    "Now that we have our model, we're ready to train it. Before we fully train the network on all our inputs, let's walk through how it works by training it with a single bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b88de-a4ca-4c8b-8757-ad8ea3773d8f",
   "metadata": {},
   "source": [
    "### Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a21bf2d-84cc-4e65-80b7-7e0bf6d26eca",
   "metadata": {},
   "source": [
    "For this example, we will train the model that when it sees the letter \"c\", it should output \"a\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c9e4e80-108f-44a9-9be4-2afe62bf80d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0188, 0.0069, 0.0147, 0.0451, 0.0446, 0.0827, 0.0278, 0.0556, 0.0163,\n",
       "        0.0092, 0.0483, 0.0047, 0.0115, 0.1003, 0.0428, 0.0021, 0.0430, 0.0579,\n",
       "        0.0272, 0.0501, 0.0473, 0.0768, 0.0168, 0.0219, 0.0561, 0.0396, 0.0316],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select weights from W to get logits.\n",
    "c_logits = c_enc @ W\n",
    "# Turn logits into probabilities with softmax.\n",
    "c_pos = c_logits.exp()\n",
    "c_probs = c_pos / c_pos.sum()\n",
    "# Examine current probabilities.\n",
    "c_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef5677ca-7c72-4812-9d1e-8ce3845be209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training, the model predicts that 'a' has a 0.69% chance of following 'c'.\n"
     ]
    }
   ],
   "source": [
    "# Check model's current prediction that \"a\" follows \"c\".\n",
    "original_prediction = c_probs[stoi[\"a\"]]\n",
    "\n",
    "print(f\"Before training, the model predicts that 'a' has a {original_prediction * 100:.2f}% chance of following 'c'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884c4caa-7e19-424b-bfad-11de9c68267a",
   "metadata": {},
   "source": [
    "That's a very low probability. To train our model, we need a way to evaluate how bad its predictions are so that we can take the necessary steps to fix them. To do this, we use something called a loss function. The goal of an ML model is to minimize the loss. For this model, the loss function we will use is called the Negative Log Likelihood (NLL) loss. Let's break down what that means.\n",
    "\n",
    "- Likelihood: Multiply together all probabilities for the chosen inputs. For this example, we only have one probability, but usually there will be many. The ideal likelihood is 1, meaning that the model always predicted 100% for the correct outputs.\n",
    "- Log: Likelihoods can be an incredibly small number, which is hard to work with since computers have limited precision. We'd have less of a small number if we could add the values instead of multiplying them. Log likelihood allows us to do just that, because $log(a*b*c) == log(a) + log(b) + log(c)$, and since log is monotonically increasing, maximizing the log likelihood is equivalent to mazimizing the likelihood, so this convenience doesn't negatively impact the outcome.\n",
    "- Negative: Likelihood is a number that we want to maximize, but loss is something that should be minimized. If we take the negative of the likelihood, it becomes something we can minimize instead, thus being useable for loss.\n",
    "\n",
    "We can then normalize that loss by dividing what we get by the total by the number of inputs. Since getting the log of the likelihood allowed us add each of the losses rather than multiplying them, the combination of adding losses and dividing by their count can be combined into a single mean() operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79ae666e-6fbd-40fc-9aeb-c2ebe2bdee22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.9747, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use NLL loss to evaluate how poorly the model currently performs.\n",
    "loss = -original_prediction.log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67becd4-6046-44c2-a880-fcaaebb1a024",
   "metadata": {},
   "source": [
    "A perfect loss would be 0. The loss we got is pretty bad. To improve our model, we'll need to change the weights of our model so they produce more accurate results, thus minimizing the loss. We won't need to change all the weights to improve this one prediction, though. We'll only need to change the weights that had an impact on the loss we got. To determine which weights to change, we use derivatives from calculus. A derivative tells us how much a small change in a variable affects the output of a function. In our case, the function is the loss function, so by calculating the derivative of the loss with respect to each weight, we can see how much each weight contributed to the loss. \n",
    "\n",
    "The collection of all these derivatives (one for each weight) is called the gradient. To find the gradient of the loss with respect to each weight, we can use backpropagation. (Remember how we created W with the parameter `requires_grad = True`? That was so we can get its gradient now.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eff85816-a368-4176-aba7-ca8cc2f8bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any existing gradient for W.\n",
    "W.grad = None\n",
    "# Backpropagate through the network to find out how much each weight impacted the loss.\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0117de3-17f4-4612-95ce-67682a2a6e14",
   "metadata": {},
   "source": [
    "Before we examine W's gradient, let's try to think through what we expect it to look like. \n",
    "\n",
    "First, let's think about the size. W is size (27, 27). Each of these weights will have a partial derivative, so W.grad should be size (27, 27) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a745ca72-780c-4458-8db2-f6896047c7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3a4400-2c0d-474c-91ea-17fd2dbd6638",
   "metadata": {},
   "source": [
    "Now, what partial derivatives should we expect? Let’s walk through the steps that lead to this loss.\n",
    "\n",
    "First, we calculated the logits with `c_enc @ W`, which multiplied our one-hot encoded vector `c_enc` (which represents \"c\" as a 1 at index 3 and 0 elsewhere) by the weights matrix `W`. As discussed earlier, multiplying a one-hot vector by a matrix selects the corresponding row of the matrix, which in this case is row 3, and multiplies each other row by 0. This means that only the weights in row 3 of `W` can have any effect on the loss. Changing the other weights can't impact the loss, since any change would still end up getting multiplied by 0.\n",
    "\n",
    "As a result, when we look at the gradient, we should expect the partial derivatives to be zero everywhere except for the weights in row 3.\n",
    "\n",
    "Let's take a look at the gradient now to see if we're on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cce13870-65df-4078-b534-2d4b01ed5e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0188, -0.9931,  0.0147,  0.0451,  0.0446,  0.0827,  0.0278,  0.0556,\n",
       "          0.0163,  0.0092,  0.0483,  0.0047,  0.0115,  0.1003,  0.0428,  0.0021,\n",
       "          0.0430,  0.0579,  0.0272,  0.0501,  0.0473,  0.0768,  0.0168,  0.0219,\n",
       "          0.0561,  0.0396,  0.0316],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2b2455-41b1-471e-a6d3-114a000dcf5b",
   "metadata": {},
   "source": [
    "Just as we expected, the only row with non-zero values is the row with index 3. \n",
    "\n",
    "The specific values of the partial derivatives in that row were determined by backpropagating through all the operations of the network, which included getting the softmax, selecting the index of \"a\" to find the model's predicted probability, and getting the Negative Log Likelihood for loss.\n",
    "\n",
    "Now let's take a closer look at the non-zero row to get more insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54a3e380-6b3b-4113-9efe-cec29f006dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0188, -0.9931,  0.0147,  0.0451,  0.0446,  0.0827,  0.0278,  0.0556,\n",
       "         0.0163,  0.0092,  0.0483,  0.0047,  0.0115,  0.1003,  0.0428,  0.0021,\n",
       "         0.0430,  0.0579,  0.0272,  0.0501,  0.0473,  0.0768,  0.0168,  0.0219,\n",
       "         0.0561,  0.0396,  0.0316])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the only row with non-zero gradients.\n",
    "W.grad[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7543660-97ef-4dca-b0f4-5f1281e56584",
   "metadata": {},
   "source": [
    "Notice how almost all of the weights in this row have a positive partial derivative with respect to the loss. That means that increasing the weights in those positions will increase the loss. We don't want that, since we're trying to minimize the loss. The only weight that has a negative partial derivative with respect to loss (meaning increasing it will decrease the loss) is the weight at index 1. This should make sense, as in this demonstration we specifically were trying to find the probability of the model predicting \"a\", which would be found at index 1 of the row selected by \"c\". If we increase that weight, we'd be increasing the probability that the model predicts for \"a\" following \"c\", so it makes perfect sense that we'd get a better result and thus decrease the loss.\n",
    "\n",
    "Now that we have a solid understanding of the gradient, we are ready to use it to improve our model. We do this by nudging each of the weights by some small amount (called the learning rate) in the direction that would decrease loss. Since decreasing the loss means changing the weights in the opposite direction of the gradient, we call this \"gradient descent\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92e7b2ab-b2a3-43e8-b4da-008d0e48fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nudge each weight in the direction opposite the gradient to minimize loss.\n",
    "W.data += -0.01 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc421ef-4b23-4940-9f2a-03bd88df8011",
   "metadata": {},
   "source": [
    "Now that we adjusted the model's weights, let's see if the model is any better at predicting that \"a\" should come after \"c\". To do this, we'll need to send our input through the network again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e456efd-c899-4489-8800-bbf08290e18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training, the model predicted that 'a' had a 0.69% chance of following 'c'.\n",
      "Now, after adjusting the weights, the model predicts that 'a' has a 0.70% chance of following 'c'.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before training, the model predicted that 'a' had a {original_prediction * 100:.2f}% chance of following 'c'.\")\n",
    "\n",
    "# Once again, select weights from W to get logits.\n",
    "c_logits = c_enc @ W\n",
    "# Turn logits into probabilities with softmax.\n",
    "c_pos = c_logits.exp()\n",
    "c_probs = c_pos / c_pos.sum()\n",
    "# Check model's current prediction that \"a\" follows \"c\".\n",
    "new_prediction = c_probs[stoi[\"a\"]]\n",
    "\n",
    "print(f\"Now, after adjusting the weights, the model predicts that 'a' has a {new_prediction * 100:.2f}% chance of following 'c'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d94f4-8c72-4c3e-8466-8bd250002151",
   "metadata": {},
   "source": [
    "As we hoped, our model is now a bit better at making predictions. To keep training, we'd need to calculate the loss again, use the new gradients for gradient descent, and keep repeating this process until the model's predictions stop improving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c819d968-ee6d-4723-a108-e649e619da8a",
   "metadata": {},
   "source": [
    "### Full training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed122e92-e6b7-4812-91a0-f9d21d700b67",
   "metadata": {},
   "source": [
    "Now that we've walked through a simple training example, we're ready to train the model on our full training data. We'll just repeat the steps we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a96056e1-11e5-441f-9f78-b638dbaf3775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Go through the full data several times to keep making improvements.\n",
    "    for _ in range(100):\n",
    "        ### Forward pass. ###\n",
    "\n",
    "        # Select row for each letter in X.\n",
    "        logits = X_enc @ W\n",
    "        # Use softmax function to turn logits into percents.\n",
    "        pos = logits.exp()\n",
    "        probs = pos / pos.sum(dim=1, keepdims=True)\n",
    "        # For each row of probs, check model's prediction for label.\n",
    "        predictions = probs[torch.arange(X_enc.size(0)), y]\n",
    "        # Use NLL loss.\n",
    "        loss = -predictions.log().mean()\n",
    "\n",
    "        ### Backwards pass. ###\n",
    "\n",
    "        # Reset gradients.\n",
    "        W.grad = None\n",
    "        # Backpropagate through full network.\n",
    "        loss.backward()\n",
    "        # Adjust weights by learning rate to improve loss.\n",
    "        W.data += -50 * W.grad\n",
    "        \n",
    "        \n",
    "    # Return loss after training so we can check model progress.\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f5f8b53-32e5-4e4b-b753-2f825c215d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking model progress every 100 training iterations.\n",
      "Loss = 2.473177194595337\n",
      "Loss = 2.4626121520996094\n",
      "Loss = 2.459376335144043\n",
      "Loss = 2.45784330368042\n",
      "Loss = 2.4569640159606934\n",
      "Loss = 2.456400156021118\n",
      "Loss = 2.4560108184814453\n",
      "Loss = 2.4557266235351562\n",
      "Loss = 2.4555113315582275\n",
      "Loss = 2.4553422927856445\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking model progress every 100 training iterations.\")\n",
    "for _ in range(10):\n",
    "    print(f\"Loss = {train()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f898926a-5ccc-44af-8d8b-d7f059b1bc5d",
   "metadata": {},
   "source": [
    "As we keep training the model, the loss will continue to decrease until the model reaches a point where it can't improve anymore. However, since training on so much data is slow, we'll stop improving the model for now and start using it to generate new, never-before-seen names!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c72b5-b3db-439d-8f5c-f17e81304ca1",
   "metadata": {},
   "source": [
    "<a id=\"generate\"></a>\n",
    "## Generating Names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa25f0-a4ac-4ed4-83f4-75955b45eff0",
   "metadata": {},
   "source": [
    "Now we're ready to use this newly trained model to generate names of its own. Remember how earlier we taught our model that each name starts after a \".\" character? This will finally come in handy here. To start generating a name, we'll feel the network the \".\" character as input. Our newly trained model should have learned which letters are most likely to follow \".\", and thus should know what letters tend to start names. If we input \".\" into the model now, we can see this for ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f8e3241-d6d4-4df8-a549-3ad1018b1f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt    # For displaying results.\n",
    "\n",
    "\n",
    "# Encode \".\" to use it as input for our model.\n",
    "encoded = F.one_hot(torch.tensor(stoi[\".\"]), num_classes=27).float()\n",
    "\n",
    "# Input \".\" into our model.\n",
    "logits = encoded @ W\n",
    "probs = logits.exp() / sum(logits.exp())    # Softmax on one line.\n",
    "\n",
    "\n",
    "# Display model's output.\n",
    "plt.bar(stoi.keys(), probs.tolist())\n",
    "plt.title(\"Distribution of First Letters in Names\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e87eaeb-1b8e-4286-8c41-3c4926c24d3b",
   "metadata": {},
   "source": [
    "It seems our model learned from the data that \"a\" is a very likely first letter, followed by \"k\". We can now use multinomial sampling to select a letter based on the weights in this distribution. This gives the model some randomness while still taking the probability of each letter into account. Let's try it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb98a0dc-7e5a-4c7a-849d-7fdf0481856a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "k\n"
     ]
    }
   ],
   "source": [
    "gen = torch.Generator().manual_seed(42)\n",
    "\n",
    "# Sample a first letter for a name 5 times.\n",
    "letter_indices = torch.multinomial(probs, num_samples=5, replacement=True, generator=gen).tolist()\n",
    "\n",
    "# Turn the indices of those letters into the actual letters.\n",
    "for index in letter_indices:\n",
    "    print(itos[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a910c8cf-6e94-47b0-9034-88ed8847bcd6",
   "metadata": {},
   "source": [
    "Those results perfectly demonstrate how the sampling works. Since \"a\" is so likely to follow \".\", it chose \"a\" four times, but we can see that it has leeway to pick other letters as well. Making the model generate a full name is the same process as making it generate the first letter. Once it generates the first letter, we send that letter through the model and select from *it's* outputs to produce a next letter, and on and on until the model finally selects a \".\", indicating the end of a name. \n",
    "\n",
    "Now we're ready to generate some names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b38d76aa-0ce1-45c2-9478-fdb591f831b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = torch.Generator().manual_seed(42)\n",
    "\n",
    "\n",
    "def generate_name():\n",
    "    name = []\n",
    "    \n",
    "    # Start with 0 (\".\") to find first letter of name.\n",
    "    letter_idx = 0\n",
    "\n",
    "    while True:\n",
    "        # Encode the letter to use in model.\n",
    "        encoded = F.one_hot(torch.tensor(letter_idx), num_classes=27).float()\n",
    "\n",
    "        # Send letter through model.\n",
    "        logits = encoded @ W\n",
    "        probs = logits.exp() / sum(logits.exp())\n",
    "\n",
    "        # Sample next letter of the name.\n",
    "        letter_idx = torch.multinomial(probs, num_samples=1, replacement=True, generator=gen).item()\n",
    "\n",
    "        # If the letter selected is \".\", we've finished the name.\n",
    "        if letter_idx == 0:\n",
    "            return \"\".join(name).capitalize()\n",
    "        \n",
    "        # Otherwise, add the new letter to the name.\n",
    "        name.append(itos[letter_idx])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "10844d02-acf7-44d2-8ba2-a8a4c7d891e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ya\n",
      "Syahavilin\n",
      "Dleekahmangonya\n",
      "Tryahe\n",
      "Chen\n",
      "Ena\n",
      "Da\n",
      "Amiiae\n",
      "A\n",
      "Keles\n"
     ]
    }
   ],
   "source": [
    "# Generate 10 names.\n",
    "for _ in range(10):\n",
    "    print(generate_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca6221f-8750-491d-9d66-fcc31165f8bc",
   "metadata": {},
   "source": [
    "Most of these generated names are pretty bad, but that is to be expected from such a simple model that relies only on bigrams. Interestingly, one name generated was Chen, which happens to be the name of a close friend of mine. \n",
    "\n",
    "A satisfying end to a fun, simple neural network exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f77b9c-5354-48b6-882b-8b098e1c3112",
   "metadata": {},
   "source": [
    "<a id=\"next\"></a>\n",
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1286776f-5e27-495c-b3fa-5d2764542ee4",
   "metadata": {},
   "source": [
    "This basic name generator was only the beginning. Next time, we'll walk through train/dev/test splits, hidden layers, and more. Stay tuned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
