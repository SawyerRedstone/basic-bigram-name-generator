{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52533478-9b0b-41b8-8b6a-e3cf89d940a7",
   "metadata": {},
   "source": [
    "# Generating Names with Bigrams\n",
    "<!-- Basic Bigram Generator -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f19b2db-e84f-44df-92a5-945800aff081",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1) [Project Overview](#overview)\n",
    "2) [Preparing the Data](#data)\n",
    "3) [Building the Bigram Model](#model)\n",
    "4) [Training the Model](#training)\n",
    "5) [Generating Names](#generate)\n",
    "6) [Next Steps](#next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2fae51-6189-4860-8e21-5a18aa9c0cea",
   "metadata": {},
   "source": [
    "<a id=\"overview\"></a>\n",
    "## Project Overview\n",
    "In this project, we will build a character-level bigram language model from scratch and use it to generate names. The model will take a letter as input and output the probability of each possible next letter. It will then use these probabilities to pick the next letter in a name, then take that next letter as input to continue generating the name one letter at a time.\n",
    "\n",
    "<!-- then feed that new letter back into the model to continue generating the name, one letter at a time. -->\n",
    "\n",
    "\n",
    "This model will use only a single layer of neurons, making it easy to see exactly how the math works under the hood.\n",
    "\n",
    "This notebook is written as both a learning exercise and a teaching tool. Every step is explained in detail, including the math behind data preparation, model predictions, loss calculation, and gradient updates. The code is intentionally kept minimal, without relying on high-level ML frameworks, so the core ideas are completely transparent.\n",
    "\n",
    "<!-- After finishing the notebook, you should understand each step  -->\n",
    "\n",
    "<!-- This notebook is both a learning exercise and a teaching tool. Each step explains what's happening, why it's done, and how it connects to later concepts in machine learning. -->\n",
    "\n",
    "<!-- The goal is to understand every step of the process, from preparing text data, to implementing the model, to training it using gradient-based optimization, to sampling from the trained model. -->\n",
    "\n",
    "<!-- All components are implemented manually, without using high-level deep learning shortcuts, so the notebook serves as both: -->\n",
    "\n",
    "<!-- The code is intentionally kept minimal, without relying on high-level ML frameworks, so the core ideas are completely transparent. -->\n",
    "\n",
    "\n",
    "<!-- \n",
    "Key features of this notebook:\n",
    "- Data preparation from raw text to numeric form.\n",
    "- Manual implementation of one-hot encoding, softmax, and loss functions.\n",
    "- Training loop with gradient calculation and parameter updates.\n",
    "- Name generation using multinomial sampling from the trained model.\n",
    " -->\n",
    "\n",
    "\n",
    "<!-- The goal of this notebook is to build a simple language model that can generate names resembling the real ones from its training data. To accomplish this goal, we'll build a simple neural network and train it on pairs of consecutive letters (bigrams) found in names. The model will be able to take a letter as input and output the probability of each possible next letter. We can then use these probabilities to pick the next letter in a name, then feed that new letter back into the model to continue generating the name, one letter at a time. -->\n",
    "\n",
    "\n",
    "<!-- In this notebook, we build a simple language model that can generate names resembling the real ones from its training data.  -->\n",
    "\n",
    "<!-- The model will be made from scratch and will use only a single layer of neurons.  -->\n",
    "\n",
    "<!-- This notebook is both a learning exercise and a teaching tool. Each step explains what's happening, why it's done, and how it connects to later concepts in machine learning. -->\n",
    "\n",
    "<!-- To make each step and simple and clear as possible, the model will be made from  -->\n",
    "\n",
    "<!-- This will be done from scratch, using only a single layer of neurons. The model should be able to take a letter as input and output the probability of each possible letter -->\n",
    "\n",
    "\n",
    "\n",
    "<!-- This notebook builds a character-level bigram language model from scratch using only a single layer of neurons. It's both a learning exercise and a teaching tool. Each step explains what's happening, why it's done, and how it connects to later concepts in machine learning. -->\n",
    "\n",
    "\n",
    "<!-- The goal of this project is to train a bigram model to generate names. -->\n",
    "\n",
    "<!-- \n",
    "\n",
    "By the end, we'll have trained a bigram model to generate names.\n",
    "\n",
    "predict the next character in a name, and use \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- This notebook builds a simple language model that generates names resembling real ones from its training data. This is done with a  -->\n",
    "\n",
    "<!-- This notebook builds a simple neural network from scratch. It's both a learning exercise and a teaching tool. Each step explains what's happening, why it's done, and how it connects to later concepts in machine learning. -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- The goal of this project is to create a simple language model that can generate names resembling real ones from its training data. -->\n",
    "\n",
    "<!-- I am making this project to review what I've been learning, but also to format it all  -->\n",
    "\n",
    "<!-- We'll use a neural network trained on character-level bigrams -- that is, pairs of consecutive letters. When the model is given a letter as input, it should predict the probability of each possible next letter. We can then use these probabilities to pick the next letter in a name, then feed that new letter back into the model to continue generating the name, one letter at a time. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced6390-bee6-4aaf-adc4-1dab1eaa6e5f",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "## Preparing the Data\n",
    "We'll start by reading in a list of names we will use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ea8ac9df-0b96-4287-9591-4a2de6feba44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the first three names: ['emma', 'olivia', 'ava']\n"
     ]
    }
   ],
   "source": [
    "# Create a list of each name in names.txt\n",
    "names = open(\"names.txt\", \"r\").read().splitlines()\n",
    "print(f\"Displaying the first three names: {names[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a74d1e7-4d8f-46c9-8fbe-58b4df1cb2c0",
   "metadata": {},
   "source": [
    "Now that we have a list of names, we need to prepare these names in a way that our model can use for training. However, we can't train on *all* the names here. If we do, then even if our model becomes amazing at generating these names, we'd have no way of knowing if it can create any realistic sounding names that weren't directly taken from the training data.\n",
    "\n",
    "\n",
    "We want a model that doesn't just memorize the data, but instead learns patterns within the data so that it can generate new names which still sound natural. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53547d50-87e3-46a9-877e-6dd4a83665b1",
   "metadata": {},
   "source": [
    "### Splitting Data into Train, Validation, and Test Sets\n",
    "\n",
    "To make sure the model doesn't just memorize the data, we need to split the data into three sets.\n",
    "1) Training Set: Used to teach the model the patterns in the data.\n",
    "2) Validation/Dev Set: Used during development to evaluate how well the model performs on data it wasn't trained with. If the performance is poor, this alerts us to adjust some of the model's settings before training it once again with the training set. \n",
    "3) Test Set: Used only once, after the model is fully trained, to see how well the finished model performs on all new, never-before-seen data.\n",
    "\n",
    "Let's split the data into these three sets now. We want each set to have a similar letter distribution, so we'll shuffle the names first in case the data was ordered any way that would bias the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "73965df6-594e-4032-b912-d7337288ba1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First three names after shuffling: ['yuheng', 'diondre', 'xavien']\n"
     ]
    }
   ],
   "source": [
    "import random    # Adds shuffling capabilities.\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(names)\n",
    "\n",
    "print(f\"First three names after shuffling: {names[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75a8cea-44b2-4ba2-aea5-ef90f1200856",
   "metadata": {},
   "source": [
    "Most of the data should be used for training the model, so we'll keep 80% for the training set, 10% for the dev set, and the remaining 10% for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0e831f82-7e83-4f0c-a21b-5463fb0706ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set contains 25626 names.\n",
      "Dev set contains 3203 names.\n",
      "Test set contains 3204 names.\n"
     ]
    }
   ],
   "source": [
    "# The training set will end at the 80% mark of all names.\n",
    "train_end_index = int(0.8 * len(names))\n",
    "train_set = names[:train_end_index]\n",
    "\n",
    "# Dev set will contain the next 10% of names, so it will end at 80 + 10 = 90% of data. \n",
    "dev_end_index = int(0.9 * len(names))\n",
    "dev_set = names[train_end_index:dev_end_index]\n",
    "\n",
    "# Remaining data will be used for test set.\n",
    "test_end_index = len(names)\n",
    "test_set = names[dev_end_index:]\n",
    "\n",
    "print(f\"Training set contains {len(train_set)} names.\")\n",
    "print(f\"Dev set contains {len(dev_set)} names.\")\n",
    "print(f\"Test set contains {len(test_set)} names.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444b7255-8b88-43ba-b2a8-a2ff441bd6ec",
   "metadata": {},
   "source": [
    "### From Names to Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3af2f8-9030-41ab-a75d-90e616d219d4",
   "metadata": {},
   "source": [
    "We want to teach our model the common letter patterns found in names. For this, we'll break each name into character-level bigrams. We also want it to learn how each name starts and ends, so let's indicate the start and end of a name with a \".\". Below shows an example of the bigrams in the name Emma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4a2761a5-07ca-4343-bf02-5b2862fadba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams for emma: [('.', 'e'), ('e', 'm'), ('m', 'm'), ('m', 'a'), ('a', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Break \"emma\" into bigrams, including \".\" for start and end.\n",
    "print(f\"Bigrams for emma: {list(zip('.emma', 'emma.'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532cd13c-5f08-43d5-abf7-e809ae7bd592",
   "metadata": {},
   "source": [
    "To use these bigrams in our neural network, we'll organize them into two tensors: X (inputs) and y (labels).\n",
    "- X will contain the first letter of each bigram. These are the inputs we'll feed into the model.\n",
    "- y will contain the second letter of each bigram, i.e., the letter that follows each input letter in X. These are the labels (targets) the model should learn to predict.\n",
    "\n",
    "\n",
    "Storing them as tensors instead of lists allows us to use them directly with our model. Let's import PyTorch for its tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "72dcc032-dbd1-4517-871e-9d48ab9549e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch    # Tensors and backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a42fb6-f8c3-4e48-b3e5-3af0feed7bea",
   "metadata": {},
   "source": [
    "Unfortunately, tensors can't contain character elements, so we can't store the bigrams directly. Instead, let's assign each letter to an integer value so we can store those instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "67fcb3e7-03f7-46f8-b54b-981b755310f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# For each letter + \".\", add them to dict with an integer value.\n",
    "stoi = {char:int_val for (int_val, char) in enumerate(\".\" + string.ascii_lowercase)}\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ee9ab-b714-4d56-9f94-95f1caa38847",
   "metadata": {},
   "source": [
    "Of course, we'll also need a way to find out which letters those numbers represent later, so we'll make another dictionary that can turn them back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ec968468-135b-48dd-a618-f2ee5b75add6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "itos = {num: letter for letter, num in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c273ea97-83a4-4cd3-8753-6e54802ec317",
   "metadata": {},
   "source": [
    "Now we are ready to split all the names into bigrams. We'll have to do it seperately for each of the sets (train, dev, and test), so we'll put the bigram code into a function to keep the code clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "353fc34a-6aac-4f6b-b64f-c4d90cac8006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set now has 182625 examples.\n",
      "The first 10 values in train_X are tensor([ 0, 25, 21,  8,  5, 14,  7,  0,  4,  9]).\n",
      "The first 10 values in train_y are tensor([25, 21,  8,  5, 14,  7,  0,  4,  9, 15]).\n"
     ]
    }
   ],
   "source": [
    "def get_bigrams(names):\n",
    "    X = [] # First letters in bigrams\n",
    "    y = [] # X's bigram pairs.\n",
    "\n",
    "    for name in names:\n",
    "        for char1, char2 in zip(\".\" + name, name + \".\"):\n",
    "            # Convert chars to ints so we can later add to tensors.\n",
    "            X.append(stoi[char1])\n",
    "            y.append(stoi[char2])\n",
    "\n",
    "    # Convert lists to tensors.\n",
    "    X = torch.tensor(X)\n",
    "    y = torch.tensor(y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "train_X, train_y = get_bigrams(train_set)\n",
    "dev_X, dev_y = get_bigrams(dev_set)\n",
    "test_X, test_y = get_bigrams(test_set)\n",
    "\n",
    "\n",
    "print(f\"Our training set now has {len(train_X)} examples.\")\n",
    "print(f\"The first 10 values in train_X are {train_X[:10]}.\")\n",
    "print(f\"The first 10 values in train_y are {train_y[:10]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772d3098-f360-4211-8ebd-0372f579c648",
   "metadata": {},
   "source": [
    "Now the data is almost ready to be used in a neural network, but there is still one issue. \n",
    "\n",
    "When you use a neural network, you feed it input examples and the network does math with those inputs to arrive at an output. Specifically, it multiplies each input by float values called \"weights\". Right now, our inputs are just numbers from 0 to 26. It wouldn't be very helpful to do math with these inputs, since we want each letter to be treated equally. There is no reason that \"z\" should be mathematically larger than \"a\", for example.\n",
    "\n",
    "To make sure each input gets treated equally, we can change our inputs with a method called **one-hot encoding**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f77c7b-76a6-4ae7-a8bc-1de0e13f734d",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc1c15a-4e0f-48b5-9805-3831086ff727",
   "metadata": {},
   "source": [
    "With one-hot encoding, each letter gets represented by an array of length 27 (one for each possible letter including \".\"). This array contains all 0s apart from a single 1 at the index signifying the chosen letter. To show this more visually, let's explore the one-hot encoded version of the letter \"c\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "808bffe7-caa8-4575-88a5-8508992ec230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c's index is 3, so its one-hot encoding looks like tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Get number used to represent c. This will be the index of the 1 after one-hot encoding.\n",
    "c_index = stoi[\"c\"]\n",
    "\n",
    "# Create the one-hot encoded representation of the letter \"c\".\n",
    "c_enc = torch.zeros(27)\n",
    "c_enc[c_index] = 1\n",
    "\n",
    "print(f\"c's index is {c_index}, so its one-hot encoding looks like {c_enc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18171a9-9ec0-4875-a966-5d376093342e",
   "metadata": {},
   "source": [
    "Let's apply this encoding to all the letters in our input data sets. Rather than encoding them all manually like above, we'll use PyTorch's one_hot() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "45a02bce-6d1e-4b42-8deb-811882958e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# One-hot encode Xs to turn each letter into an array of length 27 (one index for each possible letter including \".\".)\n",
    "train_X_enc = F.one_hot(train_X, num_classes = 27)\n",
    "dev_X_enc = F.one_hot(dev_X, num_classes = 27)\n",
    "test_X_enc = F.one_hot(test_X, num_classes = 27)\n",
    "\n",
    "\n",
    "# View encodings for first 5 letters of the training set.\n",
    "train_X_enc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85bfee4-57eb-4438-ad74-bffa495dac25",
   "metadata": {},
   "source": [
    "You may notice that the resulting tensors contain integers. To later multiply them by the float weights in our neural network, we'll need to convert those values into floats.\n",
    "\n",
    "<!-- Neural networks contain floats, so we need to convert these to floats so that the network can interact with them. -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "088e945f-df59-4662-9795-07e836c53985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want the neural net to produce floats, so the inputs must be floats as well.\n",
    "train_X_enc = train_X_enc.float()\n",
    "dev_X_enc = dev_X_enc.float()\n",
    "test_X_enc = test_X_enc.float()\n",
    "\n",
    "# The first letter in the training set is now encoded with floats.\n",
    "train_X_enc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f16bbb-cf61-4c32-91d4-fcfe5b1bdc01",
   "metadata": {},
   "source": [
    "Now that we've finished preparing the data, we're ready to build our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e5fdd6-8340-4902-9662-c7dbfc5455f4",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "## Building the Bigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6215d93b-154e-49a3-ab4a-1867125439f8",
   "metadata": {},
   "source": [
    "For this project, we'll use a very simple neural network with only a single layer of neurons. \n",
    "\n",
    "Before we jump into building this layer, it's important to understand how a single neuron works.\n",
    "\n",
    "Since each input letter now has a length of 27 (due to the encoding), this one neuron will need 27 weights so each element in the inputs can be multiplied by a unique weight. The weights will start off as random numbers from a normal distribution. (After training the model, these numbers should become more meaningful.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c7fb0bfe-1f20-448c-910d-380bd8c428c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a single neuron with a 27 weights:\n",
      "tensor([[ 0.8375],\n",
      "        [-0.2891],\n",
      "        [ 1.5804],\n",
      "        [ 0.2942],\n",
      "        [ 1.5294],\n",
      "        [ 0.0248],\n",
      "        [-1.1924],\n",
      "        [ 0.6198],\n",
      "        [ 0.6405],\n",
      "        [ 0.8103],\n",
      "        [-0.2884],\n",
      "        [-0.2535],\n",
      "        [-1.2639],\n",
      "        [ 0.7335],\n",
      "        [ 0.9185],\n",
      "        [ 1.3745],\n",
      "        [ 0.0779],\n",
      "        [ 0.0550],\n",
      "        [ 0.3096],\n",
      "        [-0.2267],\n",
      "        [-0.1192],\n",
      "        [ 0.8248],\n",
      "        [ 0.0750],\n",
      "        [-1.7949],\n",
      "        [-0.1942],\n",
      "        [ 0.6488],\n",
      "        [-0.4296]])\n"
     ]
    }
   ],
   "source": [
    "# A single neuron with one weight for each element in our input.\n",
    "W = torch.randn(27, 1)\n",
    "print(\"Here is a single neuron with a 27 weights:\")\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b5d0b-e7b8-44ea-920f-34b382eca936",
   "metadata": {},
   "source": [
    "Now we can use this neuron by multiplying our inputs with it.\n",
    "\n",
    "\n",
    "When the input is a one-hot encoded vector, the dot product multiplies all weights by 0 except for the one weight at the position where the input has a 1. Each of the resulting multiplications are then added together, but since most of the values are 0, they end up ignored by the result. Thus, it is as if each one-hot encoded input simply selects the corresponding weight from the neuron and ignores the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e9a6fe85-6afa-4ee7-9c92-623e1cde8602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_enc has a 1 at index 3, so the dot product should select the neuron's weight at index 3.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2942])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"c_enc has a 1 at index 3, so the dot product should select the neuron's weight at index 3.\")\n",
    "c_enc @ W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660294e0-4975-4045-98b4-a9464b6a097a",
   "metadata": {},
   "source": [
    "As described earlier, we want our model to take a letter as input and output the probability of each possible next letter. Since there are 27 possible next letters, the model should produce a vector of 27 probabilities, one for each letter.\n",
    "\n",
    "We've seen that a single neuron produces one output. Thus, to produce 27 outputs, we'll need a layer of 27 neurons. Since each neuron is a column of weights, 27 neurons would be represented by a tensor with 27 columns (resulting in a 27x27 matrix).\n",
    "\n",
    "We'll also need to set `requires_grad=True` when creating the tensor. This is necessary for later, as it will help us adjust the weights during training. We'll go into more detail about gradients once we get to the section on training the model.\n",
    "\n",
    "<!-- and how they are used to train the model in the next section. -->\n",
    "\n",
    "\n",
    "<!-- We'll explore what that means further once we start training\n",
    "\n",
    "Apart from having the right amount of neurons, we also need a way to update the weights during training. For that, we'll need to set `requires_grad=True` when we create the tensor. We'll explore what these means further once we get to training the model. -->\n",
    "\n",
    "\n",
    "<!-- we'll also need a way to eventually adjust the weights to train the model. We do this by setting `requires_grad=True` when we create the tensor. This allows us to update the weights during training. We'll explore what these means further once we get to training the model. -->\n",
    "<!-- \n",
    "Once we begin training\n",
    "\n",
    ", which tells PyTorch that this tensor . \n",
    "\n",
    "This tells the model to keep track of all the operations within it, \n",
    "\n",
    "This won't do anything noticeable yet, but it will be necessary later when it comes to training the model. We'll explore what  -->\n",
    "\n",
    "\n",
    "<!-- We'll also need a way to eventually adjust these weights to train the model. Later we will go in depth on how to do that, but for now, we -->\n",
    "\n",
    "<!-- We can do this with `torch.randn(27, 27)`. We'll also  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8a5caf1e-96f7-4813-b82a-a86550bd4311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights start off random, so let's use a generator.\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "\n",
    "# Each column in the weights matrix represents another neuron in the layer.\n",
    "W = torch.randn((27, 27), generator = gen, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01f61b3-0c77-4414-8478-447c26b5bf76",
   "metadata": {},
   "source": [
    "Now W (our weights) is a 27x27 matrix of random values from a normal distribution. Each column represents a single neuron in the layer. \n",
    "\n",
    "Before we explored how one-hot encoding selected a single value from the neuron. That's still true, but now that there are 27 neurons, it will select the value in that same row for each of the neurons. So for example, an input of c (index 3) dot producted with the layer would result in a vector of all the weights at the row with index 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5f033286-894f-4c58-a3a2-3498b0dc1500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3387, -1.3407, -0.5854,  0.5362,  0.5246,  1.1412,  0.0516,  0.7440,\n",
       "        -0.4816, -1.0495,  0.6039, -1.7223, -0.8278,  1.3347,  0.4835, -2.5095,\n",
       "         0.4880,  0.7846,  0.0286,  0.6408,  0.5832,  1.0669, -0.4502, -0.1853,\n",
       "         0.7528,  0.4048,  0.1785], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the third weight from each neuron.\n",
    "c_output = c_enc @ W\n",
    "c_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da32450-2be9-4486-a1d5-e6424c5e64ca",
   "metadata": {},
   "source": [
    "Now we have a model that can take a letter as input and produce 27 outputs, one for each possible next letter. \n",
    "\n",
    "You may notice that the numbers in our output don't look very much like probabilities just yet. At this stage, the outputs are just raw scores (also called \"logits\"), not probabilities. When we use this model during training and sampling, we'll need to normalize these outputs to turn them into probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "911ffc48-a3c9-4915-94b6-fb39cca04b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our untrained model's current raw scores for each letter following 'c':\n",
      ".: -0.3387\n",
      "a: -1.3407\n",
      "b: -0.5854\n",
      "c: 0.5362\n",
      "d: 0.5246\n",
      "e: 1.1412\n",
      "f: 0.0516\n",
      "g: 0.7440\n",
      "h: -0.4816\n",
      "i: -1.0495\n",
      "j: 0.6039\n",
      "k: -1.7223\n",
      "l: -0.8278\n",
      "m: 1.3347\n",
      "n: 0.4835\n",
      "o: -2.5095\n",
      "p: 0.4880\n",
      "q: 0.7846\n",
      "r: 0.0286\n",
      "s: 0.6408\n",
      "t: 0.5832\n",
      "u: 1.0669\n",
      "v: -0.4502\n",
      "w: -0.1853\n",
      "x: 0.7528\n",
      "y: 0.4048\n",
      "z: 0.1785\n"
     ]
    }
   ],
   "source": [
    "print(\"Our untrained model's current raw scores for each letter following 'c':\")\n",
    "for letter, logit in zip(stoi.keys(), c_output):\n",
    "    print(f\"{letter}: {logit:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8318672-2de8-487e-90eb-43d3882c6c18",
   "metadata": {},
   "source": [
    "Now that we understand what happens when we use a single letter as input to our neural network, let's look at what happens with the full training dataset. Our input data, stored in `train_X_enc`, contains far more than just one letter. When we pass these inputs through the network, each one-hot encoded row in `train_X_enc` selects a corresponding row from the weights matrix to produce its logits. These logits are then stacked together, resulting in an output matrix where each row contains the logits for a given input letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "97ce7428-347b-4a32-91c4-dd16c0ff89bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269e+00,  1.4873e+00,  9.0072e-01,  ...,  1.2791e+00,\n",
       "          1.2964e+00,  6.1047e-01],\n",
       "        [-1.1798e+00, -5.2974e-01,  9.6252e-01,  ..., -7.0684e-01,\n",
       "         -1.2520e+00,  3.0250e+00],\n",
       "        [-4.5008e-01,  4.7390e-01,  6.5034e-01,  ..., -9.5421e-01,\n",
       "          6.1277e-02,  8.5261e-02],\n",
       "        ...,\n",
       "        [ 2.6491e-01,  1.2732e+00, -1.3109e-03,  ..., -1.3603e-01,\n",
       "          1.6354e+00,  6.5474e-01],\n",
       "        [ 1.3347e+00, -2.3162e-01,  4.1759e-02,  ...,  5.2581e-01,\n",
       "         -4.8799e-01,  1.1914e+00],\n",
       "        [-1.2829e+00,  4.4849e-01, -5.9074e-01,  ..., -7.6019e-01,\n",
       "         -4.0751e-01,  9.6236e-01]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get logits for each letter in X.\n",
    "logits = train_X_enc @ W\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4931c7e9-68c8-4385-9b81-3a99a4e32a89",
   "metadata": {},
   "source": [
    "### Turning Logits to Probabilities with Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3682206-2be3-483c-a24d-3220fadb7131",
   "metadata": {},
   "source": [
    "Earlier, we mentioned that we'd like to turn these logits into probabilities. We can do this using the softmax function, which works like so:\n",
    "\n",
    "First, we exponentiate each of the logits. This turns all values positive while keeping their relative order. All negative numbers will turn into a value between 0 and 1, and all positive values will end up as some value larger than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1ae9a578-7c2c-4b6a-aea2-4a54de73c760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.8683,  4.4251,  2.4614,  ...,  3.5935,  3.6562,  1.8413],\n",
       "        [ 0.3074,  0.5888,  2.6183,  ...,  0.4932,  0.2859, 20.5950],\n",
       "        [ 0.6376,  1.6063,  1.9162,  ...,  0.3851,  1.0632,  1.0890],\n",
       "        ...,\n",
       "        [ 1.3033,  3.5722,  0.9987,  ...,  0.8728,  5.1316,  1.9246],\n",
       "        [ 3.7990,  0.7932,  1.0426,  ...,  1.6918,  0.6139,  3.2916],\n",
       "        [ 0.2772,  1.5659,  0.5539,  ...,  0.4676,  0.6653,  2.6179]],\n",
       "       grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change values to all be positive.\n",
    "pos_values = logits.exp()\n",
    "pos_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d970135e-e72c-460e-96e4-21829dafe893",
   "metadata": {},
   "source": [
    "Then we turn this into a probability by summing up each row and setting each logit in that row equal to their fraction of that sum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "05b6ca6b-004c-4c54-837f-ed9b72febf9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1230, 0.0793, 0.0441,  ..., 0.0644, 0.0655, 0.0330],\n",
       "        [0.0044, 0.0085, 0.0378,  ..., 0.0071, 0.0041, 0.2970],\n",
       "        [0.0184, 0.0464, 0.0554,  ..., 0.0111, 0.0307, 0.0315],\n",
       "        ...,\n",
       "        [0.0277, 0.0759, 0.0212,  ..., 0.0185, 0.1090, 0.0409],\n",
       "        [0.0934, 0.0195, 0.0256,  ..., 0.0416, 0.0151, 0.0809],\n",
       "        [0.0087, 0.0491, 0.0174,  ..., 0.0147, 0.0209, 0.0821]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum the column vectors to calculate the sum of each row, then divide each element in the row by the result to get their probabilities.\n",
    "probs = pos_values / pos_values.sum(dim=1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d5ac5b-8bb0-48d8-ba2c-b84b8b7629ff",
   "metadata": {},
   "source": [
    "If we did softmax correctly, each row should now add up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a6a9b413-5d42-4b4b-a2ed-ffa742595607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        ...,\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm that each row now sums to 100%.\n",
    "probs.sum(dim=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1106c279-8249-48a9-ab79-901e9ae9f2ab",
   "metadata": {},
   "source": [
    "<!-- Now that we applied softmax, each row of logits should  -->\n",
    "That's all there is to softmax: exponentiate to turn the logits positive, then change each logit to be a percentage of the row's sum. There is, however, one important improvement we can make before moving forward. Take a look at what happens when we exponentiate very large values in a tensor.\n",
    "\n",
    "<!-- After training, the values of each of  -->\n",
    "\n",
    "<!-- When we exponentiate the values within the logits, there is a chance that some of the resulting values will be too large for the tensor to store. For example, exponentiating 100 will result in \"inf\". That isn't very helpful for our model. -->\n",
    "\n",
    "<!-- logits, there is a chance that the resulting number will be too large for the tensor to store. -->\n",
    "\n",
    "<!-- Exponentiating a logit can cause issues, part if its value -->\n",
    "<!-- There is one potential problem with softmax that is important to adjust. *** -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5768bfd4-67b6-480d-a8da-01850f14c4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   inf, 2.5957, 1.0594, 0.9317, 1.6999, 1.4446, 2.1583, 0.8415, 0.9295,\n",
       "        5.1029, 2.0099, 0.3217, 1.1313, 0.6092, 0.4415, 2.4971, 0.7100, 0.1562,\n",
       "        0.7675, 0.5956, 0.3668, 2.0298, 2.6249, 1.2613, 0.6756, 0.8261, 1.7088])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pretend we used a one-hot encoded letter as input and got these logits as output.\n",
    "logits = torch.randn(27)\n",
    "# To spot the potential problem, we'll make the first logit very large.\n",
    "logits[0] = 100\n",
    "# Exponentiate the logits for softmax.\n",
    "logits.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb40008-5b4d-4691-8318-e1690bc4da25",
   "metadata": {},
   "source": [
    "As you can see, exponentiating 100 resulted in a number too large to store, so the resulting tensor had 'inf' instead of a true value. Luckily, there is an easy fix for this problem. If we subtract each logit by the maximum logit's value, then the largest logit becomes 0, which we can exponentiate without issue.\n",
    "\n",
    "<!-- easily store after exponentiating. -->\n",
    "\n",
    "\n",
    "<!-- \n",
    "value becomes 0, which we can easily store after exponentiating. Also, mathematically it works out that offsetting each logit by the same value doesn't impac\n",
    "\n",
    "value by the maximum in the row before exponentiating, then the largest value would be 0, which we can easily store after exponentiating. This also \n",
    "\n",
    "this will make the largest value 0. It also has \n",
    "\n",
    "we'll end up with the same result -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b983b7f4-5f74-4c74-9e85-14ebde3f3021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 9.6690e-44, 3.9236e-44, 3.5032e-44, 6.3058e-44, 5.3249e-44,\n",
       "        7.9874e-44, 3.0829e-44, 3.5032e-44, 1.8918e-43, 7.4269e-44, 1.2612e-44,\n",
       "        4.2039e-44, 2.2421e-44, 1.6816e-44, 9.2486e-44, 2.6625e-44, 5.6052e-45,\n",
       "        2.8026e-44, 2.2421e-44, 1.4013e-44, 7.5670e-44, 9.8091e-44, 4.6243e-44,\n",
       "        2.5223e-44, 3.0829e-44, 6.3058e-44])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subtract each logit by the maximum value.\n",
    "logits -= 100\n",
    "# We should no longer get 'inf' for any of the logits.\n",
    "logits.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f7783a-766e-4709-97c7-d4a9f7faba8f",
   "metadata": {},
   "source": [
    "We can do this because offsetting each logit by a constant value doesn't change the final output of the softmax function. Below we will examine the output of the softmax function before and after offsetting each logit to demonstrate that the result doesn't change.\n",
    "\n",
    "<!-- We can see this below. -->\n",
    "\n",
    "<!-- we can do it without worry. -->\n",
    "\n",
    "<!-- This offset  -->\n",
    "\n",
    "<!-- We can do this without any issues, as it mathematically works out that offsetting each logit by a constant value results in the same final output. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ce24fc67-3911-48be-9553-f2c42a46a301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before offsetting the logits, we get these probabilities for next letters:\n",
      "tensor([0.0132, 0.0082, 0.0374, 0.0173, 0.0082, 0.0238, 0.0055, 0.0730, 0.0081,\n",
      "        0.0253, 0.0057, 0.0093, 0.0917, 0.1303, 0.0523, 0.0151, 0.0233, 0.0182,\n",
      "        0.0448, 0.0107, 0.0100, 0.0069, 0.0263, 0.0267, 0.2713, 0.0124, 0.0249])\n",
      "\n",
      "After offsetting, the probabilities should be the same as before.\n",
      "tensor([0.0132, 0.0082, 0.0374, 0.0173, 0.0082, 0.0238, 0.0055, 0.0730, 0.0081,\n",
      "        0.0253, 0.0057, 0.0093, 0.0917, 0.1303, 0.0523, 0.0151, 0.0233, 0.0182,\n",
      "        0.0448, 0.0107, 0.0100, 0.0069, 0.0263, 0.0267, 0.2713, 0.0124, 0.0249])\n"
     ]
    }
   ],
   "source": [
    "# Get softmax output for random logits before offsetting anything.\n",
    "logits = torch.randn(27)\n",
    "original_probs = logits.exp() / sum(logits.exp())    # Softmax on one line.\n",
    "print(\"Before offsetting the logits, we get these probabilities for next letters:\")\n",
    "print(original_probs)\n",
    "print()\n",
    "\n",
    "# Offset each logit by maximum value, then check if output changed.\n",
    "max_value = torch.max(logits).item()\n",
    "logits -= max_value\n",
    "new_probs = logits.exp() / sum(logits.exp())    # Softmax on one line.\n",
    "print(\"After offsetting, the probabilities should be the same as before.\")\n",
    "print(new_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedc9018-019a-42ea-b527-1c075cbfc214",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"training\"></a>\n",
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1c9284-383b-4343-90e7-ac7c51b7d462",
   "metadata": {},
   "source": [
    "Now that we have all the steps for our model, we're ready to train it. Before we fully train the network on all our inputs, let's walk through how it works by training it with a single bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b88de-a4ca-4c8b-8757-ad8ea3773d8f",
   "metadata": {},
   "source": [
    "### Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a21bf2d-84cc-4e65-80b7-7e0bf6d26eca",
   "metadata": {},
   "source": [
    "For this example, we will train the model that when it sees the letter \"c\", it should output \"a\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8c9e4e80-108f-44a9-9be4-2afe62bf80d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0188, 0.0069, 0.0147, 0.0451, 0.0446, 0.0827, 0.0278, 0.0556, 0.0163,\n",
       "        0.0092, 0.0483, 0.0047, 0.0115, 0.1003, 0.0428, 0.0021, 0.0430, 0.0579,\n",
       "        0.0272, 0.0501, 0.0473, 0.0768, 0.0168, 0.0219, 0.0561, 0.0396, 0.0316],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select weights from W to get logits.\n",
    "c_logits = c_enc @ W\n",
    "# Offset logits by max value.\n",
    "c_logits -= torch.max(c_logits).item()\n",
    "# Turn logits into probabilities with softmax.\n",
    "c_pos = c_logits.exp()\n",
    "c_probs = c_pos / c_pos.sum()\n",
    "# Examine current probabilities.\n",
    "c_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ef5677ca-7c72-4812-9d1e-8ce3845be209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training, the model predicts that 'a' has a 0.69% chance of following 'c'.\n"
     ]
    }
   ],
   "source": [
    "# Check model's current prediction that \"a\" follows \"c\".\n",
    "original_prediction = c_probs[stoi[\"a\"]]\n",
    "\n",
    "print(f\"Before training, the model predicts that 'a' has a {original_prediction * 100:.2f}% chance of following 'c'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884c4caa-7e19-424b-bfad-11de9c68267a",
   "metadata": {},
   "source": [
    "That's a very low probability. To train our model, we need a way to evaluate how bad its predictions are so that we can take the necessary steps to fix them. To do this, we use something called a loss function. The goal of an ML model is to minimize the loss. For this model, the loss function we will use is called the Negative Log Likelihood (NLL) loss. Let's break down what that means.\n",
    "\n",
    "- Likelihood: The probability that the model assigns to the correct output. For a single input, this is just the model's predicted probability for the true label. For multiple independent inputs, we multiply the predicted probabilities for each correct label to get the total likelihood of all predictions being correct. The better a model performs, the higher its likelihood will be, with a perfect model having a likelihood of 1 (meaning it predicted 100% probability for each correct label).\n",
    "- Log Likelihood: Likelihood values can become extremely small when multiplying many probabilities, which is hard for computers to work with due to limited precision. Taking the log solves this, because $log(a*b*c) == log(a) + log(b) + log(c)$. This replaces multiplication with addition, which is more numerically stable. Since the log function is monotonically increasing, maximizing the log likelihood is equivalent to maximizing the likelihood.\n",
    "- Negative Log Likelihood (NLL): Loss functions are minimized, not maximized. By taking the *negative* of the log likelihood, we turn our maximization problem into a minimization problem, making it usable as a loss function. \n",
    "\n",
    "\n",
    "We can normalize that loss by dividing what we get by the total by the number of inputs. Since getting the log of the likelihood allowed us add each of the losses rather than multiplying them, the combination of adding losses and dividing by their count can be combined into a single `mean()` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "79ae666e-6fbd-40fc-9aeb-c2ebe2bdee22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.9747, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use NLL loss to evaluate how poorly the model currently performs.\n",
    "loss = -original_prediction.log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67becd4-6046-44c2-a880-fcaaebb1a024",
   "metadata": {},
   "source": [
    "A perfect loss would be 0. The loss we got here is pretty bad. To improve our model, we need to adjust its weights so the predictions become more accurate, thus minimizing the loss. We don't need to change all the weights to improve this one prediction, though. We only need to change the ones that influenced the loss. To determine which weights to change (and by how much), we use derivatives from calculus. A derivative tells us how much a small change in a variable affects the output of a function. In our case, the function is the loss function. By calculating the derivative of the loss with respect to each weight, we can see how much each weight contributed to that loss. \n",
    "\n",
    "\n",
    "The collection of all these derivatives (one for each weight) is called the gradient. We compute the gradient using backpropagation. (Remember when we created W with `requires_grad=True`? That's what allows PyTorch to store and give us its gradient now.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "eff85816-a368-4176-aba7-ca8cc2f8bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any existing gradient for W.\n",
    "W.grad = None\n",
    "# Backpropagate through the network to find out how much each weight impacted the loss.\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0117de3-17f4-4612-95ce-67682a2a6e14",
   "metadata": {},
   "source": [
    "Before we examine W's gradient, let's try to think through what we expect it to look like. \n",
    "\n",
    "First, let's think about the size. W is size (27, 27). Each of these weights will have a partial derivative, so W.grad should be size (27, 27) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a745ca72-780c-4458-8db2-f6896047c7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3a4400-2c0d-474c-91ea-17fd2dbd6638",
   "metadata": {},
   "source": [
    "Now, what partial derivatives should we expect? Let’s walk through the steps that lead to this loss.\n",
    "\n",
    "First, we calculated the logits with `c_enc @ W`, which multiplied our one-hot encoded vector `c_enc` (which represents \"c\" as a 1 at index 3 and 0 elsewhere) by the weights matrix `W`. As discussed earlier, multiplying a one-hot vector by a matrix selects the corresponding row of the matrix, which in this case is row 3, and multiplies each other row by 0. This means that only the weights in row 3 of `W` can have any effect on the loss. Changing the other weights can't impact the loss, since any change would still end up getting multiplied by 0.\n",
    "\n",
    "As a result, when we look at the gradient, we should expect the partial derivatives to be zero everywhere except for the weights in row 3.\n",
    "\n",
    "Let's take a look at the gradient now to see if we're on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cce13870-65df-4078-b534-2d4b01ed5e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0188, -0.9931,  0.0147,  0.0451,  0.0446,  0.0827,  0.0278,  0.0556,\n",
       "          0.0163,  0.0092,  0.0483,  0.0047,  0.0115,  0.1003,  0.0428,  0.0021,\n",
       "          0.0430,  0.0579,  0.0272,  0.0501,  0.0473,  0.0768,  0.0168,  0.0219,\n",
       "          0.0561,  0.0396,  0.0316],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2b2455-41b1-471e-a6d3-114a000dcf5b",
   "metadata": {},
   "source": [
    "Just as we expected, the only row with non-zero values is the row with index 3. \n",
    "\n",
    "The specific values of the partial derivatives in that row were determined by backpropagating through all the operations of the network, which included getting the softmax, selecting the index of \"a\" to find the model's predicted probability, and getting the Negative Log Likelihood for loss.\n",
    "\n",
    "Now let's take a closer look at the non-zero row to get more insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "54a3e380-6b3b-4113-9efe-cec29f006dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0188, -0.9931,  0.0147,  0.0451,  0.0446,  0.0827,  0.0278,  0.0556,\n",
       "         0.0163,  0.0092,  0.0483,  0.0047,  0.0115,  0.1003,  0.0428,  0.0021,\n",
       "         0.0430,  0.0579,  0.0272,  0.0501,  0.0473,  0.0768,  0.0168,  0.0219,\n",
       "         0.0561,  0.0396,  0.0316])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the only row with non-zero gradients.\n",
    "W.grad[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7543660-97ef-4dca-b0f4-5f1281e56584",
   "metadata": {},
   "source": [
    "Notice how almost all of the weights in this row have a positive partial derivative with respect to the loss. That means that increasing the weights in those positions will increase the loss. We don't want that, since we're trying to minimize the loss. The only weight that has a negative partial derivative with respect to loss (meaning increasing it will decrease the loss) is the weight at index 1. This should make sense, as in this demonstration we specifically were trying to find the probability of the model predicting \"a\", which would be found at index 1 of the row selected by \"c\". If we increase that weight, we'd be increasing the probability that the model predicts for \"a\" following \"c\", so it makes perfect sense that we'd get a better result and thus decrease the loss.\n",
    "\n",
    "Now that we have a solid understanding of the gradient, we are ready to use it to improve our model. We do this by nudging each of the weights by some small amount (called the learning rate) in the direction that would decrease loss. Since decreasing the loss means changing the weights in the opposite direction of the gradient, we call this \"gradient descent\". \n",
    "\n",
    "<!-- For now, we'll use a learning rate of 0.01. Later, we'll determine the best learning rate for our problem. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "92e7b2ab-b2a3-43e8-b4da-008d0e48fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a small number for the learning rate.\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Nudge each weight in the direction opposite the gradient to minimize loss.\n",
    "W.data -= learning_rate * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc421ef-4b23-4940-9f2a-03bd88df8011",
   "metadata": {},
   "source": [
    "Now that we adjusted the model's weights, let's see if the model is any better at predicting that \"a\" should come after \"c\". To do this, we'll need to send our input through the network again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4e456efd-c899-4489-8800-bbf08290e18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training, the model predicted that 'a' had a 0.69% chance of following 'c'.\n",
      "Now, after adjusting the weights, the model predicts that 'a' has a 0.70% chance of following 'c'.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before training, the model predicted that 'a' had a {original_prediction * 100:.2f}% chance of following 'c'.\")\n",
    "\n",
    "# Once again, select weights from W to get logits.\n",
    "c_logits = c_enc @ W\n",
    "# Offset logits to prevent overflow.\n",
    "c_logits -= torch.max(c_logits).item()\n",
    "# Turn logits into probabilities with softmax.\n",
    "c_pos = c_logits.exp()\n",
    "c_probs = c_pos / c_pos.sum()\n",
    "# Check model's current prediction that \"a\" follows \"c\".\n",
    "new_prediction = c_probs[stoi[\"a\"]]\n",
    "\n",
    "print(f\"Now, after adjusting the weights, the model predicts that 'a' has a {new_prediction * 100:.2f}% chance of following 'c'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d94f4-8c72-4c3e-8466-8bd250002151",
   "metadata": {},
   "source": [
    "As we hoped, the model is now a bit better at making predictions. To keep training, we'd need to calculate the loss again, use the new gradients for gradient descent, and keep repeating this process until the model's predictions stop improving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c819d968-ee6d-4723-a108-e649e619da8a",
   "metadata": {},
   "source": [
    "### Training on Full Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d8a4a-cb30-4bae-86f3-f4df68bb07bd",
   "metadata": {},
   "source": [
    "<!-- Now that we've walked through a simple training example, let's train the model with all the examples from train_X at once. We'll just repeat the steps we did above. -->\n",
    "\n",
    "Now that we've walked through a simple training example, we're ready to train the model on our full training data. We'll just repeat the steps we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7300e388-1650-48ba-b8a9-877da06467fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 3.696241617202759\n",
      "loss = 3.695483684539795\n",
      "loss = 3.694725513458252\n",
      "loss = 3.6939687728881836\n",
      "loss = 3.6932129859924316\n",
      "loss = 3.6924571990966797\n",
      "loss = 3.6917028427124023\n",
      "loss = 3.6909494400024414\n",
      "loss = 3.6901957988739014\n",
      "loss = 3.6894431114196777\n"
     ]
    }
   ],
   "source": [
    "# Go through the full training data several times to keep making improvements.\n",
    "for _ in range(10):\n",
    "    ### Forward pass. ###\n",
    "\n",
    "    # Select row for each letter in train_X.\n",
    "    logits = train_X_enc @ W\n",
    "    # Offset to prevent overflow. Max function returns (values, indices), so only select values.\n",
    "    logits -= torch.max(logits, dim=1, keepdim=True)[0]\n",
    "    # Use softmax function to turn logits into percents.\n",
    "    pos = logits.exp()\n",
    "    probs = pos / pos.sum(dim=1, keepdims=True)\n",
    "    # For each row of probs, check model's prediction for label.\n",
    "    predictions = probs[torch.arange(train_X_enc.size(0)), train_y]\n",
    "    # Use NLL loss.\n",
    "    loss = -predictions.log().mean()\n",
    "    # Track loss to see it improve with each iteration.\n",
    "    print(f\"loss = {loss.item()}\")\n",
    "\n",
    "    ### Backward pass. ###\n",
    "\n",
    "    # Reset gradients.\n",
    "    W.grad = None\n",
    "    # Backpropagate through full network.\n",
    "    loss.backward()\n",
    "    # Adjust weights by learning rate to improve loss.\n",
    "    W.data += -0.1 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18445ede-be4c-42d9-93fb-9a14c8e10af5",
   "metadata": {},
   "source": [
    "As we keep training, the loss will decrease until the model reaches a point where it can't improve further. \n",
    "\n",
    "<!-- So far, we've only run 10 iterations (nudging the weights just 10 times) which is far from enough to fully train a model. However, updating the weights using the entire training set for every adjustment can be slow, especially as the dataset grows. Each iteration would require processing all examples, making training increasingly time-consuming. To speed things up, we can train on minibatches, which are small subsets of the data. This allows us to make more frequent weight updates while still moving each weight in the right direction. -->\n",
    "\n",
    "<!-- As we keep training the model, the loss will continue to decrease until the model reaches a point where it can't improve anymore. So far we ran it for 10 iterations, which means that it only nudged the weights 10 times. That is not nearly enough to fully train a model. But making the model go through every letter of the training data each time we want to slightly adjust the weights can be very time consuming, and the larger the dataset, the slower each iteration takes. Luckily, we can speed up the training process substantially using minibatches. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abfe8e5-1a98-4587-9dc1-f9ae87207706",
   "metadata": {},
   "source": [
    "### Training with Minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfaeffd-67a2-4c4f-b0a7-923398308d4a",
   "metadata": {},
   "source": [
    "So far, we've only run 10 iterations (nudging the weights just 10 times) which is far from enough to fully train a model. However, updating the weights using the entire training set for every adjustment can be slow, especially as the dataset grows. Each iteration would require processing all examples, making training increasingly time-consuming. To speed things up, we can train on minibatches, which are small subsets of the data. This allows us to make more frequent weight updates while still moving each weight in the right direction. Let's try that now. (I'll rewrite the code in different words this time. Use whichever helps make the concepts more clear to you.)\n",
    "\n",
    "<!-- Let's try training using minibatches this time. We can do this by randomly selecting some examples from our training data each iteration and only training on that selection. We'll put the code in a function now so we can use it many times and test how well it performs.  -->\n",
    "\n",
    "<!-- We'll also use `lr` as a parameter to the function so we can  -->\n",
    "\n",
    "<!-- To train with minibatches, we randomly select some examples from our training data each iteration and only train on that selection for the iteration. -->\n",
    "\n",
    "\n",
    "<!-- We can do this by making a list of length `minibatch_size` where each element is a random index in X. Then we can use that to index on train_X_enc and train on that. -->\n",
    "\n",
    "\n",
    "\n",
    "<!-- Since we \n",
    "\n",
    "- Make list of length minibatch_size where each element is a random index in X. Use that to index X and train on that. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "31dbea76-cf9e-460f-8750-b80162a9d15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for _ in range(1000):\n",
    "        # Pick 32 random indices of train_X to go in our batch.\n",
    "        batch_indices = torch.randint(0, train_X_enc.size(0), (32,), generator=gen)\n",
    "        # Select each of the random 32 rows (each row indicating a single one-hot encoded letter).\n",
    "        batch = train_X_enc[batch_indices]\n",
    "        # Select their matching labels.\n",
    "        batch_labels = train_y[batch_indices]\n",
    "        \n",
    "        #### Forward Pass ####\n",
    "        \n",
    "        logits = batch @ W\n",
    "        # Offset each row by max number to prevent overflow.\n",
    "        logits -= torch.max(logits, dim=1, keepdim=True)[0]\n",
    "        # Softmax to get probabilities of following letters.\n",
    "        probs = logits.exp() / torch.sum(logits.exp(), dim=1, keepdims=True)\n",
    "        # Check probability assigned to labels to see how well model performed.\n",
    "        label_probs = probs[torch.arange(32), batch_labels]\n",
    "        # Get loss using Negative Log Likelihood and normalize result.\n",
    "        loss = -label_probs.log().mean()\n",
    "        \n",
    "        #### Backwards Pass ####\n",
    "        \n",
    "        # Always reset the gradient before backpropagation!\n",
    "        W.grad = None\n",
    "        # Backpropagate through the network.\n",
    "        loss.backward()\n",
    "        # Use gradient descent to nudge weights by the learning rate.\n",
    "        W.data -= 0.1 * W.grad\n",
    "        \n",
    "    # Return loss found before the final adjustment.\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ba3b1f75-a33b-4e72-a466-73c8b35f4b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking model progress every 1000 training iterations.\n",
      "Loss = 3.08286452293396\n",
      "Loss = 2.9375686645507812\n",
      "Loss = 2.663137197494507\n",
      "Loss = 2.4567668437957764\n",
      "Loss = 2.806462526321411\n",
      "Loss = 2.6709558963775635\n",
      "Loss = 2.5657105445861816\n",
      "Loss = 2.7251834869384766\n",
      "Loss = 2.481252670288086\n",
      "Loss = 2.3794548511505127\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking model progress every 1000 training iterations.\")\n",
    "\n",
    "# Go through 1000 iterations 10 times to see impovements.\n",
    "for _ in range(10):\n",
    "    print(f\"Loss = {train()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae88d5-fa83-453e-8085-40aec121cb4a",
   "metadata": {},
   "source": [
    "TODO: Continue here\n",
    "\n",
    "Model loss actually occasionally rose before falling back down. (Test on dev set + then discuss finding optimal learning rate?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f898926a-5ccc-44af-8d8b-d7f059b1bc5d",
   "metadata": {},
   "source": [
    "As we keep training the model, the loss will continue to decrease until the model reaches a point where it can't improve anymore. However, since training on so much data is slow, we'll stop improving the model for now and start using it to generate new, never-before-seen names!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c72b5-b3db-439d-8f5c-f17e81304ca1",
   "metadata": {},
   "source": [
    "<a id=\"generate\"></a>\n",
    "## Generating Names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa25f0-a4ac-4ed4-83f4-75955b45eff0",
   "metadata": {},
   "source": [
    "Now we're ready to use this newly trained model to generate names of its own. Remember how earlier we taught our model that each name starts after a \".\" character? This will finally come in handy here. To start generating a name, we'll feel the network the \".\" character as input. Our newly trained model should have learned which letters are most likely to follow \".\", and thus should know what letters tend to start names. If we input \".\" into the model now, we can see this for ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8e3241-d6d4-4df8-a549-3ad1018b1f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt    # For displaying results.\n",
    "\n",
    "\n",
    "# Encode \".\" to use it as input for our model.\n",
    "encoded = F.one_hot(torch.tensor(stoi[\".\"]), num_classes=27).float()\n",
    "\n",
    "# Input \".\" into our model.\n",
    "logits = encoded @ W\n",
    "probs = logits.exp() / sum(logits.exp())    # Softmax on one line.\n",
    "\n",
    "\n",
    "# Display model's output.\n",
    "plt.bar(stoi.keys(), probs.tolist())\n",
    "plt.title(\"Distribution of First Letters in Names\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e87eaeb-1b8e-4286-8c41-3c4926c24d3b",
   "metadata": {},
   "source": [
    "It seems our model learned from the data that \"a\" is a very likely first letter, followed by \"k\". We can now use multinomial sampling to select a letter based on the weights in this distribution. This gives the model some randomness while still taking the probability of each letter into account. Let's try it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb98a0dc-7e5a-4c7a-849d-7fdf0481856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting generator so cell always gives same letters instead of generating the next letters.\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "\n",
    "# Sample a first letter for a name 5 times.\n",
    "letter_indices = torch.multinomial(probs, num_samples=5, replacement=True, generator=gen).tolist()\n",
    "\n",
    "# Turn the indices of those letters into the actual letters.\n",
    "for index in letter_indices:\n",
    "    print(itos[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a910c8cf-6e94-47b0-9034-88ed8847bcd6",
   "metadata": {},
   "source": [
    "Those results perfectly demonstrate how the sampling works. Since \"a\" is so likely to follow \".\", it chose \"a\" four times, but we can see that it has leeway to pick other letters as well. Making the model generate a full name is the same process as making it generate the first letter. Once it generates the first letter, we send that letter through the model and select from *it's* outputs to produce a next letter, and on and on until the model finally selects a \".\", indicating the end of a name. \n",
    "\n",
    "Now we're ready to generate some names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38d76aa-0ce1-45c2-9478-fdb591f831b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = torch.Generator().manual_seed(42)\n",
    "\n",
    "\n",
    "def generate_name():\n",
    "    name = []\n",
    "    \n",
    "    # Start with 0 (\".\") to find first letter of name.\n",
    "    letter_idx = 0\n",
    "\n",
    "    while True:\n",
    "        # Encode the letter to use in model.\n",
    "        encoded = F.one_hot(torch.tensor(letter_idx), num_classes=27).float()\n",
    "\n",
    "        # Send letter through model.\n",
    "        logits = encoded @ W\n",
    "        probs = logits.exp() / sum(logits.exp())\n",
    "\n",
    "        # Sample next letter of the name.\n",
    "        letter_idx = torch.multinomial(probs, num_samples=1, replacement=True, generator=gen).item()\n",
    "\n",
    "        # If the letter selected is \".\", we've finished the name.\n",
    "        if letter_idx == 0:\n",
    "            return \"\".join(name).capitalize()\n",
    "        \n",
    "        # Otherwise, add the new letter to the name.\n",
    "        name.append(itos[letter_idx])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10844d02-acf7-44d2-8ba2-a8a4c7d891e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 10 names.\n",
    "for _ in range(10):\n",
    "    print(generate_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca6221f-8750-491d-9d66-fcc31165f8bc",
   "metadata": {},
   "source": [
    "Most of these generated names are pretty bad, but that is to be expected from such a simple model that relies only on bigrams. Interestingly, one name generated was Chen, which happens to be the name of a close friend of mine. \n",
    "\n",
    "A satisfying end to a fun, simple neural network exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f77b9c-5354-48b6-882b-8b098e1c3112",
   "metadata": {},
   "source": [
    "<a id=\"next\"></a>\n",
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1286776f-5e27-495c-b3fa-5d2764542ee4",
   "metadata": {},
   "source": [
    "This basic name generator was only the beginning. Next time, we'll walk through hidden layers, embeddings, and more. Stay tuned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
