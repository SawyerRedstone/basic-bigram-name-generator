{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52533478-9b0b-41b8-8b6a-e3cf89d940a7",
   "metadata": {},
   "source": [
    "# Generating Names with Bigrams\n",
    "<!-- Basic Bigram Generator -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f19b2db-e84f-44df-92a5-945800aff081",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1) [Project Overview](#overview)\n",
    "2) [Preparing the Data](#data)\n",
    "3) [Building the Bigram Model](#model)\n",
    "4) [Training the Model](#training)\n",
    "5) [Generating Names](#generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2fae51-6189-4860-8e21-5a18aa9c0cea",
   "metadata": {},
   "source": [
    "<a id=\"overview\"></a>\n",
    "## Project Overview \n",
    "The goal of this project is to create a simple language model that can generate names resembling real ones from its training data.\n",
    "\n",
    "We'll use a neural network trained on character-level bigrams -- that is, pairs of consecutive letters. When the model is given a letter as input, it should predict the probability of each possible next letter. We can then use these probabilities to pick the next letter in a name, then feed that new letter back into the model to continue generating the name, one letter at a time.\n",
    "\n",
    "\n",
    "<!-- There will be two main steps for completing this goal. First, we'll train the model so it learns common bigrams in names. Next, we'll use that trained model to generate new, never-seen-before names. -->\n",
    "\n",
    "\n",
    "<!-- To put that simply, if this model is given a letter, it should learn the probabilities for each letter following that one so that it can produce a likely next letter.  -->\n",
    "\n",
    "<!-- \n",
    "learn which letter might \n",
    "\n",
    "it should find the percent chance that every other \n",
    "\n",
    "this model should learn the likelihood \n",
    "\n",
    "for any letter, the model should output the chance of each other letter following that one in a name.\n",
    "\n",
    "Given a letter, the model should output the probability of each other letter following that one in a name.\n",
    "\n",
    "That is, it will learn the common sequences of two adjacent letters found in names and use that to generate letters that might reasonably follow any given letter in a name. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced6390-bee6-4aaf-adc4-1dab1eaa6e5f",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "## Preparing the Data\n",
    "<!-- We'll be training this model on names so it learns to produce names of its own based on what it learns from the data. First we'll import our dependencies. -->\n",
    "<!-- We'll be storing our data in tensors, so let's start by importing PyTorch.\n",
    "\n",
    "Let's start by importing PyTorch so we can store our data in tensors (and later use  -->\n",
    "<!-- First we'll grab a list of names to use for training. -->\n",
    "We'll start by reading in a list of names we will use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea8ac9df-0b96-4287-9591-4a2de6feba44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the first three names: ['emma', 'olivia', 'ava']\n"
     ]
    }
   ],
   "source": [
    "# Create a list of each name in names.txt\n",
    "names = open(\"names.txt\", \"r\").read().splitlines()\n",
    "print(f\"Displaying the first three names: {names[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2689af61-4b56-4456-a36e-4062f2b94d39",
   "metadata": {},
   "source": [
    "Now that we have our data, let's split it into bigrams so the model can learn common letter patterns in names. We also want it to learn how each name starts and ends, so let's indicate the start and end of a name with a \".\". Below shows an example of the bigrams in the name Emma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a2761a5-07ca-4343-bf02-5b2862fadba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams for emma: [('.', 'e'), ('e', 'm'), ('m', 'm'), ('m', 'a'), ('a', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Break \"emma\" into bigrams, including \".\" for start and end.\n",
    "print(f\"Bigrams for emma: {list(zip('.emma', 'emma.'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532cd13c-5f08-43d5-abf7-e809ae7bd592",
   "metadata": {},
   "source": [
    "<!-- We are going to store these bigrams in tensors to use them in our neural network. -->\n",
    "To use these bigrams in our neural network, we'll organize them into two tensors: X (inputs) and y (labels).\n",
    "<!-- X will contain each first letter within the bigrams. When we train our model, X will be the input.\n",
    "y will contain the second letters within the bigrams, i.e., the solutions to the bigrams started in X. Throughout training, our model should learn to predict the values in y given X. -->\n",
    "- X will contain the first letter of each bigram. These are the inputs we'll feed into the model.\n",
    "- y will contain the second letter of each bigram, i.e., the letter that follows each input letter in X. These are the labels or \"targets\" the model should learn to predict.\n",
    "\n",
    "To start off, let's import Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72dcc032-dbd1-4517-871e-9d48ab9549e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch    # Tensors and backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a42fb6-f8c3-4e48-b3e5-3af0feed7bea",
   "metadata": {},
   "source": [
    "Since PyTorch tensors can't contain character elements, let's assign each letter to an integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67fcb3e7-03f7-46f8-b54b-981b755310f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# For each letter + \".\", add them to dict with an integer value.\n",
    "stoi = {char:int_val for (int_val, char) in enumerate(\".\" + string.ascii_lowercase)}\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ee9ab-b714-4d56-9f94-95f1caa38847",
   "metadata": {},
   "source": [
    "We'll also want a way to go backwards, turning numbers back into letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec968468-135b-48dd-a618-f2ee5b75add6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "itos = {num: letter for letter, num in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c273ea97-83a4-4cd3-8753-6e54802ec317",
   "metadata": {},
   "source": [
    "Now we are ready to split all the names into bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "353fc34a-6aac-4f6b-b64f-c4d90cac8006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training data has 228146 examples.\n",
      "The first 10 values in X are [0, 5, 13, 13, 1, 0, 15, 12, 9, 22].\n",
      "The first 10 values in y are [5, 13, 13, 1, 0, 15, 12, 9, 22, 9].\n"
     ]
    }
   ],
   "source": [
    "X = [] # First letters in bigrams\n",
    "y = [] # X's bigram pairs.\n",
    "\n",
    "for name in names:\n",
    "    for char1, char2 in zip(\".\" + name, name + \".\"):\n",
    "        # Convert chars to ints so we can later add to tensors.\n",
    "        X.append(stoi[char1])\n",
    "        y.append(stoi[char2])\n",
    "\n",
    "print(f\"Our training data has {len(X)} examples.\")\n",
    "print(f\"The first 10 values in X are {X[:10]}.\")\n",
    "print(f\"The first 10 values in y are {y[:10]}.\")\n",
    "\n",
    "# Convert list to a tensor.\n",
    "X = torch.tensor(X)\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc1c15a-4e0f-48b5-9805-3831086ff727",
   "metadata": {},
   "source": [
    "Before we can use these values in a neural network, we'll need to alter them a bit more. See, neural networks contain weights which get multiplied by each input. Right now our inputs are just numbers from 0 to 26. It wouldn't be very helpful to do math with these inputs, since we want each letter to be treated equally. \"z\" shouldn't have a different multiplier than \"a\", for example.\n",
    "\n",
    "To make sure each input gets treated equally, we can use one-hot encoding. With one-hot encoding, each letter gets represented by an array of length 27 (one for each possible letter including \".\"). This array contains all 0s apart from a single 1 at the index signifying the chosen letter. To show this more visually, let's explore the one-hot encoded version of the letter \"c\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "808bffe7-caa8-4575-88a5-8508992ec230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c's index is 3, so its one-hot encoding looks like tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Get number used to represent c. This will be the index of the 1 after one-hot encoding.\n",
    "c_index = stoi[\"c\"]\n",
    "\n",
    "# Create the one-hot encoded representation of the letter \"c\".\n",
    "c_enc = torch.zeros(27)\n",
    "c_enc[c_index] = 1\n",
    "\n",
    "print(f\"c's index is {c_index}, so its one-hot encoding looks like {c_enc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18171a9-9ec0-4875-a966-5d376093342e",
   "metadata": {},
   "source": [
    "Let's apply this encoding to all the letters in our input data. Rather than encoding them all manually like above, we'll use PyTorch's one_hot() function. \n",
    "\n",
    "<!-- Note that the function makes the datatype of the encodings integers. To use them in our neural network\n",
    "\n",
    "Unlike when I created the encoding myself, however, this will default to making the datatype of each element an integer.  -->\n",
    "\n",
    "<!-- This function doesn't automatically keep the values as floats -->\n",
    "\n",
    "<!-- By default, this function will store the letters  -->\n",
    "<!-- That way, they will all be represented similarly -->\n",
    "\n",
    "<!-- We'll also turn the values into floats while we're at it, as this is necessary so that they can be multiplied by float weights in our neural network. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45a02bce-6d1e-4b42-8deb-811882958e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# One-hot encode X to turn each letter into an array of length 27 (one index for each possible letter including \".\".)\n",
    "X_enc = F.one_hot(X, num_classes = 27)\n",
    "# View encodings for first 5 letters.\n",
    "X_enc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85bfee4-57eb-4438-ad74-bffa495dac25",
   "metadata": {},
   "source": [
    "As you can see, the values here are all integers. To later multiply them by the float weights in our neural network, we'll need to convert those values into floats.\n",
    "\n",
    "<!-- We'll also turn the values into floats while we're at it, as this is necessary so that they can be multiplied by float weights in our neural network. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "088e945f-df59-4662-9795-07e836c53985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want the neural net to produce floats, so the inputs must be floats as well.\n",
    "X_enc = X_enc.float()\n",
    "# The first letter should now be encoded with floats.\n",
    "X_enc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e5fdd6-8340-4902-9662-c7dbfc5455f4",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "## Building the Bigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6215d93b-154e-49a3-ab4a-1867125439f8",
   "metadata": {},
   "source": [
    "<!-- Now that we've finished preparing our data, we're ready to make our neural network. We won't be training it just yet; this section is more about exploring how neural networks work and then showing how we can use it to generate names once it is trained. -->\n",
    "\n",
    "<!-- Our neural network  -->\n",
    "For this project, we'll use a very simple neural network with only a single layer of neurons. \n",
    "\n",
    "<!-- Before creating this model, let's explore how a single neuron works. -->\n",
    "\n",
    "Before we jump into building this layer, it's important to understand how a single neuron works.\n",
    "\n",
    "Since each letter in X now has a length of 27 (due to the encoding), this one neuron will need 27 weights so each element in the inputs can be multiplied by a unique weight. The weights will start off as random numbers from a normal distribution. (After training the model, these numbers should become more meaningful.)\n",
    "\n",
    "\n",
    "<!-- A single neuron wouldn't be a very good network, but we will start with that to understand what exactly is happening in a neural net. -->\n",
    "\n",
    "<!-- Before we get to making our full neural network, we'll start by exploring how a single neuron works. A neuron is  -->\n",
    "<!-- A single neuron wouldn't be a very good network, but it is important to understand how it functions before making  -->\n",
    "\n",
    "<!-- understanding it is -->\n",
    "\n",
    "<!-- we will start with that to understand what exactly is happening in a neural net. -->\n",
    "\n",
    "\n",
    "<!-- for now, we will just explore what a neuron even is, then  -->\n",
    "\n",
    "<!-- this section is more about exploring how neural networks work and then showing how we can use it to generate names once it is trained. -->\n",
    "\n",
    "<!-- for now, we'll just be making the untrained model and walking through how we can use it. -->\n",
    "\n",
    "<!-- we will just be exploring what a neural network is and how it will be used to make predictions.\n",
    "\n",
    "works.\n",
    "\n",
    "TODO: Should we show how the model will make predictions here? -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7fb0bfe-1f20-448c-910d-380bd8c428c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a single neuron with a 27 weights:\n",
      "tensor([[ 1.9269],\n",
      "        [ 1.4873],\n",
      "        [ 0.9007],\n",
      "        [-2.1055],\n",
      "        [ 0.6784],\n",
      "        [-1.2345],\n",
      "        [-0.0431],\n",
      "        [-1.6047],\n",
      "        [-0.7521],\n",
      "        [ 1.6487],\n",
      "        [-0.3925],\n",
      "        [ 0.2415],\n",
      "        [-1.1109],\n",
      "        [ 0.0915],\n",
      "        [-2.3169],\n",
      "        [-0.2168],\n",
      "        [-1.3847],\n",
      "        [-0.8712],\n",
      "        [-0.2234],\n",
      "        [-0.6216],\n",
      "        [-0.5920],\n",
      "        [-0.0631],\n",
      "        [-0.8286],\n",
      "        [ 0.3309],\n",
      "        [-1.5576],\n",
      "        [ 0.9956],\n",
      "        [-0.8798]])\n"
     ]
    }
   ],
   "source": [
    "# Weights start off random, so let's use a generator.\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "\n",
    "# A single neuron with one weight for each element in our input.\n",
    "W = torch.randn((27, 1), generator=gen)\n",
    "print(\"Here is a single neuron with a 27 weights:\")\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b5d0b-e7b8-44ea-920f-34b382eca936",
   "metadata": {},
   "source": [
    "<!-- TODO: Finish rewording this. (And maybe change W's name since it isn't a full network? But it is still weights, so maybe keep it?) -->\n",
    "\n",
    "<!-- To use a neural network, we multiply  -->\n",
    "\n",
    "<!-- Now that we have a neuron, let's try using it with our input to how it works. To use a neural network, we multiply each input by the weights within its neurons. -->\n",
    "<!-- To use a neural network, we multiply each input by the weights within its neurons. Here we only have one neuron, so \n",
    "\n",
    "We can use a neural network by multiplying each input \n",
    "\n",
    "Let's try using this neuron! As mentioned earlier, we use this network by multiplying each input by the weights of this neuron.\n",
    " -->\n",
    "Now we can use this neuron by multiplying our inputs with it.\n",
    "\n",
    "\n",
    "When the input is a one-hot encoded vector, the dot product multiplies all weights by 0 except for the one weight at the position where the input has a 1. Each of the resulting multiplications are then added together, but since most of the values are 0, they end up ignored by the result. Thus, it is as if each one-hot encoded input simply selects the corresponding weight from the neuron and ignores the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9a6fe85-6afa-4ee7-9c92-623e1cde8602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_enc has a 1 at index 3, so the dot product should select the neuron's weight at index 3.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-2.1055])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"c_enc has a 1 at index 3, so the dot product should select the neuron's weight at index 3.\")\n",
    "c_enc @ W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660294e0-4975-4045-98b4-a9464b6a097a",
   "metadata": {},
   "source": [
    "<!-- Now let's think for a moment about the ideal behavior of our neural network. As mentioned in the overview, we'd like our model to take a letter as input and determine the probability for each possible next letter. Since there are 27 possible options for next letters, our model should produce 27 outputs for each input letter (where each output indicates the probability of the letter at that index coming next). -->\n",
    "\n",
    "As described earlier, we want our model to take a letter as input and output the probability of each possible next letter. Since there are 27 possible next letters, the model should produce a vector of 27 probabilities, one for each letter.\n",
    "\n",
    "We've seen that a single neuron produces one output. Thus, to produce 27 outputs, we'll need a layer of 27 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a5caf1e-96f7-4813-b82a-a86550bd4311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting generator so cell always gives same numbers instead of generating the next numbers.\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "\n",
    "# Each column in the weights matrix represents another neuron in the layer.\n",
    "W = torch.randn((27, 27), generator = gen, requires_grad=True)    # `requires_grad` will be explained later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01f61b3-0c77-4414-8478-447c26b5bf76",
   "metadata": {},
   "source": [
    "Now W (our weights) is a 27x27 matrix of random values from a normal distribution. Each column represents a single neuron in the layer. \n",
    "\n",
    "Before we explored how one-hot encoding selected a single value from the neuron. That's still true, but now that there are 27 neurons, it will select the value in that same row for each of the neurons. So for example, an input of c (index 3) dot producted with the layer would result in a vector of all the weights at the row with index 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f033286-894f-4c58-a3a2-3498b0dc1500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3387, -1.3407, -0.5854,  0.5362,  0.5246,  1.1412,  0.0516,  0.7440,\n",
       "        -0.4816, -1.0495,  0.6039, -1.7223, -0.8278,  1.3347,  0.4835, -2.5095,\n",
       "         0.4880,  0.7846,  0.0286,  0.6408,  0.5832,  1.0669, -0.4502, -0.1853,\n",
       "         0.7528,  0.4048,  0.1785], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the third weight from each neuron.\n",
    "c_output = c_enc @ W\n",
    "c_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da32450-2be9-4486-a1d5-e6424c5e64ca",
   "metadata": {},
   "source": [
    "Now we have a model that can take a letter as input and produce 27 outputs, one for each possible next letter. \n",
    "\n",
    "You may notice that the numbers in our output don't look very much like probabilities just yet. At this stage, the outputs are just raw scores (also called \"logits\"), not probabilities. When we use this model during training and sampling, we'll need to normalize these outputs to turn them into probabilities. \n",
    "\n",
    "<!-- Below shows each letter and its associated logit. -->\n",
    "\n",
    "<!-- To make it more clear how each logit is associated with a letter,  -->\n",
    "\n",
    "<!-- To make this more clear, below we can see how each value in this output vector corresponds to a specific letter. -->\n",
    "\n",
    "<!-- Each value in this output vector corresponds to a specific letter and can eventually be used to find their probability.  -->\n",
    "\n",
    "<!-- Let's match up each possible next letter with the value -->\n",
    "\n",
    "<!-- Ideally, we'd like these values to represent the probabilities of each letter coming after \"c\". However, so far, the outputs are just raw scores (called \"logits\"), not probabilities.  -->\n",
    "\n",
    "<!-- Logits must be converted into \n",
    "\n",
    "\n",
    "Now we have a model that outputs 27 values for each input letter, but these values aren't yet probabilities. Instead, what we have are logits\n",
    "\n",
    "So far what we have are \n",
    "\n",
    "\n",
    "Now we have a model that can take a letter as input and produce 27 outputs, one for each possible next letter. These values aren't probabilities just yet. So far, we only \n",
    "\n",
    "Now we have a model that can take a letter as input and produce 27 outputs, one for each possible next letter. Each value in this output vector corresponds to a specific letter and can eventually be used to find their probability. \n",
    "\n",
    "\n",
    "After training the model, these numbers can be used to find the probability that each \n",
    "\n",
    "These aren't yet the probabilities of \n",
    "\n",
    "Ideally, we'd like these values to represent the probabilities of each letter coming after \"c\". However, so far, the outputs are just raw scores (also called \"logits\"), not probabilities.\n",
    "\n",
    "\n",
    "Ideally, we'd like these values to represent the probabilities of each letter coming after \"c\". Of course, our model isn't trained yet, so we can't expect the probabilities to be correct just yet. But if we take a look at the \n",
    "\n",
    "these numbers should still be random. But \n",
    "\n",
    "\n",
    "\n",
    "But let's take a closer look at these values before going further.\n",
    "\n",
    "Ideally, we'd like these values to represent the probabilities of each letter coming after \"c\". However, so far, the outputs are just raw scores (also called \"logits\"), not probabilities.\n",
    "\n",
    "Just to make it more clear, let's \n",
    "\n",
    "<!-- Ideally, we'd like each of these outputs to represent the probability that the letter at that index comes after \"c\". To make this clearer, let's match up each output with the letter it represents. -->\n",
    "\n",
    "<!-- let's match up each possible next letter with the value  -->\n",
    "\n",
    "<!-- we can match each of these values with the letter it coincides with. -->\n",
    "\n",
    "<!-- However, you may notice that the numbers in our output don't look very much like probabilities just yet. --> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "911ffc48-a3c9-4915-94b6-fb39cca04b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our untrained model's current raw scores for each letter following 'c':\n",
      ".: -0.3387\n",
      "a: -1.3407\n",
      "b: -0.5854\n",
      "c: 0.5362\n",
      "d: 0.5246\n",
      "e: 1.1412\n",
      "f: 0.0516\n",
      "g: 0.7440\n",
      "h: -0.4816\n",
      "i: -1.0495\n",
      "j: 0.6039\n",
      "k: -1.7223\n",
      "l: -0.8278\n",
      "m: 1.3347\n",
      "n: 0.4835\n",
      "o: -2.5095\n",
      "p: 0.4880\n",
      "q: 0.7846\n",
      "r: 0.0286\n",
      "s: 0.6408\n",
      "t: 0.5832\n",
      "u: 1.0669\n",
      "v: -0.4502\n",
      "w: -0.1853\n",
      "x: 0.7528\n",
      "y: 0.4048\n",
      "z: 0.1785\n"
     ]
    }
   ],
   "source": [
    "print(\"Our untrained model's current raw scores for each letter following 'c':\")\n",
    "for letter, logit in zip(stoi.keys(), c_output):\n",
    "    print(f\"{letter}: {logit:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8318672-2de8-487e-90eb-43d3882c6c18",
   "metadata": {},
   "source": [
    "<!-- Of course, our data (stored in X_enc) contains much more than just a single letter. If our input contains many letters (such as all the letters stored in X), the output will still produce a vector for each letter in the input, but each of these vectors will be combined as rows of a matrix. Thus, if we send the network all of X as input, the resulting output will be a matrix with a row of probabilities for each of the 228146 examples letter in X. -->\n",
    "\n",
    "Now that we understand what happens when we use a single letter as input to our neural network, let's look at what happens with the full dataset. Our input data, stored in `X_enc`, contains far more than just one letter. When we pass these inputs through the network, each one-hot encoded row in `X_enc` selects a corresponding row from the weights matrix to produce its logits. These logits are then stacked together, resulting in an output matrix where each row contains the logits for one input letter.\n",
    "\n",
    "<!-- \n",
    "Now we understand what happens when we use a single letter as input to our neural network. Of course, our data (stored in `X_enc`) contains much more than just a single letter. Each of the 228146 rows in `X_enc` represent an input letter, and they each use their one-hot encoding to select a row from the weights matrix for their logits. These logits are stacked in rows and outputted together as a matrix.\n",
    "\n",
    "as rows in a matrix.\n",
    "\n",
    "has its own one-hot encoding and selects a row of weights from the matrix, just like `c_enc`.\n",
    "\n",
    "\n",
    "When we input many letters at once (for example, all the letters in `X`), each \n",
    "\n",
    "the network will output a vector of logits for each input letter. These vectors are stacked together as rows in a matrix. Thus, if we pass all of `X` as input, the output will be a matrix where each row contains the logits for the next letter, corresponding to each of the 228146 input letters in `X`. -->\n",
    "\n",
    "<!-- , but multiplying `X_enc @ W` will result in a vector just like above for each letter in X. The only difference is that instead of the network outputting a single vector,  -->\n",
    "\n",
    "<!-- . When we use this neural network on all the letters in X, we'll get a matrix where each row is a vector just like above  -->\n",
    "\n",
    "<!-- , but multiplying `X_enc @ W` will result in a vector just like above for each letter in X. These outputs  -->\n",
    "<!-- Each of these vectors will be stored as a row in a final output matrix. -->\n",
    "\n",
    "<!-- Now that we have a model that can produce 27 outputs for each input, we're on the right track. However, you may notice that the numbers in our output don't look very much like probabilities just yet. -->\n",
    "\n",
    "<!-- Now let's think about these output vectors a bit. We want each output to tell us the probability of each other letter following the input letter. Right now, the numbers in our output don't look very much like probabilities. For starters, some of them are negative. We can fix that using exponentiation, which shifts numbers over so that negative numbers end up as small decimal values and positive numbers end up larger than 1. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97ce7428-347b-4a32-91c4-dd16c0ff89bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873,  0.9007,  ...,  1.2791,  1.2964,  0.6105],\n",
       "        [ 0.5760,  1.1415,  0.0186,  ...,  0.8123, -1.9006,  0.2286],\n",
       "        [-0.3303, -0.7939,  0.3752,  ...,  0.6854, -0.1397, -1.1808],\n",
       "        ...,\n",
       "        [-1.1798, -0.5297,  0.9625,  ..., -0.7068, -1.2520,  3.0250],\n",
       "        [ 1.3463,  0.8556,  0.3220,  ..., -1.4740, -0.3502,  0.4590],\n",
       "        [ 0.4557,  0.2503, -1.3611,  ...,  2.1296, -1.5181,  0.1387]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get logits for each letter in X.\n",
    "logits = X_enc @ W\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4931c7e9-68c8-4385-9b81-3a99a4e32a89",
   "metadata": {},
   "source": [
    "### Turning Logits to Probabilities with Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3682206-2be3-483c-a24d-3220fadb7131",
   "metadata": {},
   "source": [
    "Earlier, we mentioned that we'd like to turn these logits into probabilities. We can do this using the softmax function, which works like so:\n",
    "\n",
    "First, we exponentiate each of the logits. This turns all values positive while keeping their relative order. All negative numbers will turn into a value between 0 and 1, and all positive values will end up as some value larger than 1.\n",
    "\n",
    "<!-- , as negative numbers will still be smaller than positive numbers. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ae9a578-7c2c-4b6a-aea2-4a54de73c760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.8683,  4.4251,  2.4614,  ...,  3.5935,  3.6562,  1.8413],\n",
       "        [ 1.7789,  3.1315,  1.0187,  ...,  2.2532,  0.1495,  1.2568],\n",
       "        [ 0.7187,  0.4521,  1.4553,  ...,  1.9845,  0.8696,  0.3070],\n",
       "        ...,\n",
       "        [ 0.3074,  0.5888,  2.6183,  ...,  0.4932,  0.2859, 20.5950],\n",
       "        [ 3.8430,  2.3528,  1.3799,  ...,  0.2290,  0.7045,  1.5825],\n",
       "        [ 1.5773,  1.2844,  0.2564,  ...,  8.4117,  0.2191,  1.1488]],\n",
       "       grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change values to all be positive.\n",
    "pos_values = logits.exp()\n",
    "pos_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d970135e-e72c-460e-96e4-21829dafe893",
   "metadata": {},
   "source": [
    "Then we turn this into a probability by summing up each row and setting each logit in that row equal to their fraction of that sum. \n",
    "<!-- This process is called the softmax function. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05b6ca6b-004c-4c54-837f-ed9b72febf9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1230, 0.0793, 0.0441,  ..., 0.0644, 0.0655, 0.0330],\n",
       "        [0.0396, 0.0698, 0.0227,  ..., 0.0502, 0.0033, 0.0280],\n",
       "        [0.0123, 0.0078, 0.0250,  ..., 0.0340, 0.0149, 0.0053],\n",
       "        ...,\n",
       "        [0.0044, 0.0085, 0.0378,  ..., 0.0071, 0.0041, 0.2970],\n",
       "        [0.0843, 0.0516, 0.0303,  ..., 0.0050, 0.0154, 0.0347],\n",
       "        [0.0435, 0.0354, 0.0071,  ..., 0.2319, 0.0060, 0.0317]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum the column vectors to calculate the sum of each row, then divide each element in the row by the result to get their probabilities.\n",
    "probs = pos_values / pos_values.sum(dim=1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d5ac5b-8bb0-48d8-ba2c-b84b8b7629ff",
   "metadata": {},
   "source": [
    "If we did softmax correctly, each row should now add up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6a9b413-5d42-4b4b-a2ed-ffa742595607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        ...,\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm that each row now sums to 100%.\n",
    "probs.sum(dim=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedc9018-019a-42ea-b527-1c075cbfc214",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"training\"></a>\n",
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1c9284-383b-4343-90e7-ac7c51b7d462",
   "metadata": {},
   "source": [
    "Now that we have our model, we're ready to train it. Before we fully train the network on all our inputs, let's walk through how it works by training it with a single bigram.\n",
    "\n",
    "<!-- \"s\" \"a\" -->\n",
    "\n",
    "<!-- , we'll walk through how it works by training it with a single made-up bigram. -->\n",
    "\n",
    "<!-- let's walk through the steps of training it with a single input. -->\n",
    "\n",
    "<!-- When we train the model, we change the weights of its neurons so that the logits \n",
    "\n",
    "Training the model is \n",
    "\n",
    "Right now, when we feed out model \n",
    "\n",
    "\n",
    "Our goal with training is simple: when we give it an input letter, \n",
    "To train our model, we're going to try using it on X.\n",
    "\n",
    "Once our model is well trained, it should give the correct Y values high probabilities. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b88de-a4ca-4c8b-8757-ad8ea3773d8f",
   "metadata": {},
   "source": [
    "### Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a21bf2d-84cc-4e65-80b7-7e0bf6d26eca",
   "metadata": {},
   "source": [
    "<!-- Since my name is Sawyer, for this example we will train the model that when it sees the letter \"s\", it should output \"a\". -->\n",
    "For this example, we will train the model that when it sees the letter \"c\", it should output \"a\". \n",
    "\n",
    "<!-- First, let's check what percent chance the model currently thinks \n",
    "how the model currently predicts \n",
    "\n",
    "see the probability the model currently assigns \"a\" \n",
    " -->\n",
    "<!-- we need to check how well it currently does  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c9e4e80-108f-44a9-9be4-2afe62bf80d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0188, 0.0069, 0.0147, 0.0451, 0.0446, 0.0827, 0.0278, 0.0556, 0.0163,\n",
       "        0.0092, 0.0483, 0.0047, 0.0115, 0.1003, 0.0428, 0.0021, 0.0430, 0.0579,\n",
       "        0.0272, 0.0501, 0.0473, 0.0768, 0.0168, 0.0219, 0.0561, 0.0396, 0.0316],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select weights from W to get logits.\n",
    "c_logits = c_enc @ W\n",
    "# Turn logits into probabilities with softmax.\n",
    "c_pos = c_logits.exp()\n",
    "c_probs = c_pos / c_pos.sum()\n",
    "# Examine current probabilities.\n",
    "c_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef5677ca-7c72-4812-9d1e-8ce3845be209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training, the model predicts that 'a' has a 0.69% chance of following 'c'.\n"
     ]
    }
   ],
   "source": [
    "# Check model's current prediction that \"a\" follows \"c\".\n",
    "original_prediction = c_probs[stoi[\"a\"]]\n",
    "\n",
    "print(f\"Before training, the model predicts that 'a' has a {original_prediction * 100:.2f}% chance of following 'c'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884c4caa-7e19-424b-bfad-11de9c68267a",
   "metadata": {},
   "source": [
    "That's a very low probability. To train our model, we need a way to evaluate how bad its predictions are so that we can take the necessary steps to fix them. To do this, we use something called a loss function. The goal of an ML model is to minimize the loss. For this model, the loss function we will use is called the Negative Log Likelihood (NLL) loss. Let's break down what that means.\n",
    "\n",
    "- Likelihood: Multiply together all probabilities for the chosen inputs. For this example, we only have one probability, but usually there will be many. The ideal likelihood is 1, meaning that the model always predicted 100% for the correct outputs.\n",
    "- Log: Likelihoods can be an incredibly small number, which is hard to work with since computers have limited precision. We'd have less of a small number if we could add the values instead of multiplying them. Log likelihood allows us to do just that, because $log(a*b*c) == log(a) + log(b) + log(c)$, and since log is monotonically increasing, maximizing the log likelihood is equivalent to mazimizing the likelihood, so this convenience doesn't negatively impact the outcome.\n",
    "- Negative: Likelihood is a number that we want to maximize, but loss is something that should be minimized. If we take the negative of the likelihood, it becomes something we can minimize instead, thus being useable for loss.\n",
    "\n",
    "We can then normalize that loss by dividing what we get by the total by the number of inputs. Since getting the log of the likelihood allowed us add each of the losses rather than multiplying them, the combination of adding losses and dividing by their count can be combined into a single mean() operation.\n",
    "\n",
    "\n",
    "<!-- Once we get to training our model on many examples at once, we can also normalize that loss by dividing the total by the number of inputs. Since using log causes the  -->\n",
    "\n",
    "\n",
    "<!-- work with more than one prediction at a time, we'll also normalize the results by  -->\n",
    "\n",
    "\n",
    "<!-- Maximum Likelihood Estimation (MLE). With MLE,  -->\n",
    "\n",
    "<!-- When we training our model, we need a way to evaluate how bad its predictions are so that we can take the necessary steps to fix them. -->\n",
    "\n",
    "<!-- were so it can take the steps -->\n",
    "\n",
    "\n",
    "<!-- \n",
    "Before we can adjust the weights to fix it, however, we need a way to evaluate how wrong this pro\n",
    "\n",
    "In order to evaluate just how \n",
    "\n",
    "When we're training our model, we'll need a way for the model to evaluate how bad its predictions were so it can take the steps\n",
    "\n",
    "We need a way for our model to evaluate just how bad its prediction was so that it knows how much to change the weights by to improve it. For that, we need a loss function. Here we'll use a variation of Maximum Likelihood Estimation as our loss. Specifically, we will use the negative log likelihood loss. Let's break down what that means. -->\n",
    "\n",
    "\n",
    "\n",
    "<!-- Here we will use the negative log likelihood loss. This is a  -->\n",
    "\n",
    "<!-- We'll base our loss off of the likelihood loss.  -->\n",
    "\n",
    "\n",
    "<!-- Here we will use the average negative log likelihood loss. Let's break down what that means.\n",
    "\n",
    "- Likelihood: Multiply together all probabilities for the chosen inputs. For this example, we only have one probability, but usually there will be many. The ideal likelihood is 1, meaning that the model always predicted 100% for the correct output.\n",
    "- Log: Likelihoods can be an incredibly small number, which is hard to work with since computers have limited precision. We'd have less of a small number if we could add the values instead of multiplying them. Log likelihood allows us to do just that, because $log(a*b*c) == log(a) + log(b) + log(c)$, and since log is monotonically increasing, maximizing the log likelihood is equivalent to mazimizing the likelihood, so this convenience doesn't negatively impact the outcome.\n",
    "<!-- Taking the log of it helps turn that number into a value that is easier to manage. -->\n",
    "<!-- - Log: Likelihood says to multiply all the outputs. That's  -->\n",
    "<!-- - Negative: Likelihood is a number that we want to maximize, but loss is something that should be minimized. If we take the negative of the likelihood, it becomes something we can minimize instead, thus being useable for loss. -->\n",
    "<!-- - Average: We want to know how well our model predicts overall for all our bigrams. For that, we get the average. -->\n",
    "<!-- - * Normalize it by dividing by the number of examples. -->\n",
    "\n",
    "\n",
    "<!-- This multiplies together all the  -->\n",
    "\n",
    "<!-- We need to change the weights of our model to improve them. To do that, first we need a way to tell the model just how bad its prediction was. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79ae666e-6fbd-40fc-9aeb-c2ebe2bdee22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.9747, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use NLL loss to evaluate how poorly the model currently performs.\n",
    "loss = -original_prediction.log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67becd4-6046-44c2-a880-fcaaebb1a024",
   "metadata": {},
   "source": [
    "<!-- Remember, this loss is a result of our model's weights  -->\n",
    "\n",
    "\n",
    "A perfect loss would be 0. The loss we got is pretty bad. To improve our model, we'll need to change the weights of our model so they produce more accurate results, thus minimizing the loss. We won't need to change all the weights to improve this one prediction, though. We'll only need to change the weights that had an impact on the loss we got. To determine which weights to change, we use derivatives from calculus. A derivative tells us how much a small change in a variable affects the output of a function. In our case, the function is the loss function, so by calculating the derivative of the loss with respect to each weight, we can see how much each weight contributed to the loss. \n",
    "\n",
    "The collection of all these derivatives (one for each weight) is called the gradient. To find the gradient of the loss with respect to each weight, we can use backpropagation. (Remember how we created W with the parameter `requires_grad = True`? That was so we can get its gradient now.) \n",
    "<!-- PyTorch tensors conveniently have backpropagation -->\n",
    "\n",
    "\n",
    "\n",
    "<!-- we can find out how much the loss will change if we nudge that weight. -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- So now we need a way to find out which weights of the model we need to change to improve  -->\n",
    "\n",
    "<!-- Now that we know the loss, we need to change the weights of our model in such a way to minimize that loss. For that, we need some way to determine which weights affected the loss we got. -->\n",
    "\n",
    "\n",
    "<!-- To minimize this loss, we need some method of determining  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eff85816-a368-4176-aba7-ca8cc2f8bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any existing gradient for W.\n",
    "W.grad = None\n",
    "# Backpropagate through the network to find out how much each weight impacted the loss.\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0117de3-17f4-4612-95ce-67682a2a6e14",
   "metadata": {},
   "source": [
    "Before we examine W's gradient, let's try to think through what we expect it to look like. \n",
    "\n",
    "First, let's think about the size. W is size (27, 27). Each of these weights will have a partial derivative, so W.grad should be size (27, 27) as well.\n",
    "<!-- \n",
    ", so we expect the gradient to be \n",
    "\n",
    "that same size, as each weight in W will have a derivative assigned to it.\n",
    "\n",
    "We know that each weight will get assigned a \n",
    "\n",
    "\n",
    "Let's think through the steps that lead to the eventual loss to see if we can determine which weights had an impact on it.\n",
    "\n",
    "First, we  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a745ca72-780c-4458-8db2-f6896047c7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 27])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3a4400-2c0d-474c-91ea-17fd2dbd6638",
   "metadata": {},
   "source": [
    "Now, what partial derivatives should we expect? Letâ€™s walk through the steps that lead to this loss.\n",
    "\n",
    "First, we calculated the logits with `c_enc @ W`, which multiplied our one-hot encoded vector `c_enc` (which represents \"c\" as a 1 at index 3 and 0 elsewhere) by the weights matrix `W`. As discussed earlier, multiplying a one-hot vector by a matrix selects the corresponding row of the matrix, which in this case is row 3, and multiplies each other row by 0. This means that only the weights in row 3 of `W` can have any effect on the loss. Changing the other weights can't impact the loss, since any change would still end up getting multiplied by 0.\n",
    "\n",
    "As a result, when we look at the gradient, we should expect the partial derivatives to be zero everywhere except for the weights in row 3.\n",
    "\n",
    "Let's take a look at the gradient now to see if we're on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cce13870-65df-4078-b534-2d4b01ed5e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0188, -0.9931,  0.0147,  0.0451,  0.0446,  0.0827,  0.0278,  0.0556,\n",
       "          0.0163,  0.0092,  0.0483,  0.0047,  0.0115,  0.1003,  0.0428,  0.0021,\n",
       "          0.0430,  0.0579,  0.0272,  0.0501,  0.0473,  0.0768,  0.0168,  0.0219,\n",
       "          0.0561,  0.0396,  0.0316],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2b2455-41b1-471e-a6d3-114a000dcf5b",
   "metadata": {},
   "source": [
    "Just as we expected, the only row with non-zero values is the row with index 3. \n",
    "\n",
    "The specific values of the partial derivatives in that row were determined by backpropagating through all the operations of the network, which included getting the softmax, selecting the index of \"a\" to find the model's predicted probability, and getting the Negative Log Likelihood for loss.\n",
    "\n",
    "<!-- which included getting the softmax and then finding the model's predicted probability for \"a\" by selecting the value at index 1 of the result, as well as all the loss operations. -->\n",
    "\n",
    "<!-- selecting the value at in\n",
    "\n",
    "resulting weight in index 1 for \"a\". -->\n",
    "\n",
    "<!-- Now, we won't go so far as calculating the specific partial derivatives shown above, but we *can* get a bit more insight into where those numbers came from. When we selected \"a\", that was in index 1   -->\n",
    "\n",
    "Now let's take a closer look at the non-zero row to get more insight.\n",
    "<!-- Now, we won't go so far as calculating the specific values shown above, but we *can* get a bit more insight into where those numbers came from. Remember, each column in the weights matrix represents one neuron  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54a3e380-6b3b-4113-9efe-cec29f006dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0188, -0.9931,  0.0147,  0.0451,  0.0446,  0.0827,  0.0278,  0.0556,\n",
       "         0.0163,  0.0092,  0.0483,  0.0047,  0.0115,  0.1003,  0.0428,  0.0021,\n",
       "         0.0430,  0.0579,  0.0272,  0.0501,  0.0473,  0.0768,  0.0168,  0.0219,\n",
       "         0.0561,  0.0396,  0.0316])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the only row with non-zero gradients.\n",
    "W.grad[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7543660-97ef-4dca-b0f4-5f1281e56584",
   "metadata": {},
   "source": [
    "Notice how almost all of the weights in this row have a positive partial derivative with respect to the loss. That means that increasing the weights in those positions will increase the loss. We don't want that, since we're trying to minimize the loss. The only weight that has a negative partial derivative with respect to loss (meaning increasing it will decrease the loss) is the weight at index 1. This should make sense, as in this demonstration we specifically were trying to find the probability of the model predicting \"a\", which would be found at index 1 of the row selected by \"c\". If we increase that weight, we'd be increasing the probability that the model predicts for \"a\" following \"c\", so it makes perfect sense that we'd get a better result and thus decrease the loss.\n",
    "\n",
    "Now that we have a solid understanding of the gradient, we are ready to use it to improve our model. We do this by nudging each of the weights by some small amount (called the learning rate) in the direction that would decrease loss. Since decreasing the loss means changing the weights in the opposite direction of the gradient, we call this \"gradient descent\". \n",
    "\n",
    "<!-- \n",
    "Naturally, if that probability were higher, \n",
    "\n",
    "<!-- These partial derivatives tell us how much the loss will increase if we increase the given weight. \n",
    "\n",
    "Notice how only one of the weights in this row has a negative partial derivative with respect to the loss. That means that increasing that weight will decrease the loss by the amount shown. \n",
    "\n",
    "<!-- These values are the selected weights' partial derivatives with respect to the loss.\n",
    "\n",
    "That means that the weights with positive values w\n",
    "\n",
    "These partial derivatives tell us how much the loss will increase if we increase their weights.\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92e7b2ab-b2a3-43e8-b4da-008d0e48fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nudge each weight in the direction opposite the gradient to minimize loss.\n",
    "W.data += -0.01 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc421ef-4b23-4940-9f2a-03bd88df8011",
   "metadata": {},
   "source": [
    "Now that we adjusted the model's weights, let's see if the model is any better at predicting that \"a\" should come after \"c\". To do this, we'll need to send our input through the network again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e456efd-c899-4489-8800-bbf08290e18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training, the model predicted that 'a' had a 0.69% chance of following 'c'.\n",
      "Now, after adjusting the weights, the model predicts that 'a' has a 0.70% chance of following 'c'.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before training, the model predicted that 'a' had a {original_prediction * 100:.2f}% chance of following 'c'.\")\n",
    "\n",
    "# Once again, select weights from W to get logits.\n",
    "c_logits = c_enc @ W\n",
    "# Turn logits into probabilities with softmax.\n",
    "c_pos = c_logits.exp()\n",
    "c_probs = c_pos / c_pos.sum()\n",
    "# Check model's current prediction that \"a\" follows \"c\".\n",
    "new_prediction = c_probs[stoi[\"a\"]]\n",
    "\n",
    "print(f\"Now, after adjusting the weights, the model predicts that 'a' has a {new_prediction * 100:.2f}% chance of following 'c'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d94f4-8c72-4c3e-8466-8bd250002151",
   "metadata": {},
   "source": [
    "As we hoped, our model is now a bit better at making predictions. To keep training, we'd need to calculate the loss again, use the new gradients for gradient descent, and keep repeating this process until the model's predictions stop improving.\n",
    "<!-- \n",
    "repeat the \n",
    "\n",
    "\n",
    "The improvement isn't immense, since we only nudged the weights by a small amount, but when we go through the full training data we'll continously iterate through the steps of making predictions, calculating the new loss, then nudging the weights by a small amount \n",
    "\n",
    "We only made one slight adjustment to the weights here, but  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c819d968-ee6d-4723-a108-e649e619da8a",
   "metadata": {},
   "source": [
    "### Full training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed122e92-e6b7-4812-91a0-f9d21d700b67",
   "metadata": {},
   "source": [
    "Now that we've walked through a simple training example, we're ready to train the model on our full training data. We'll just repeat the steps we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a96056e1-11e5-441f-9f78-b638dbaf3775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Go through the full data several times to keep making improvements.\n",
    "    for _ in range(100):\n",
    "        ### Forward pass. ###\n",
    "\n",
    "        # Select row for each letter in X.\n",
    "        logits = X_enc @ W\n",
    "        # Use softmax function to turn logits into percents.\n",
    "        pos = logits.exp()\n",
    "        probs = pos / pos.sum(dim=1, keepdims=True)\n",
    "        # For each row of probs, check model's prediction for label.\n",
    "        predictions = probs[torch.arange(X_enc.size(0)), y]\n",
    "        # Use NLL loss.\n",
    "        loss = -predictions.log().mean()\n",
    "\n",
    "        ### Backwards pass. ###\n",
    "\n",
    "        # Reset gradients.\n",
    "        W.grad = None\n",
    "        # Backpropagate through full network.\n",
    "        loss.backward()\n",
    "        # Adjust weights by learning rate to improve loss.\n",
    "        W.data = -50 * W.grad\n",
    "        \n",
    "    # Return loss after training so we can check model progress.\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f5f8b53-32e5-4e4b-b753-2f825c215d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.085660934448242"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71312672-67a5-4efd-b204-6a36fda89fcf",
   "metadata": {},
   "source": [
    "TODO: Add model smoothing, then generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "729ba862-325c-4cd7-9a70-4d93723397bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try input letter from X.\n",
    "# Softmax the output.\n",
    "# Use y to find current probability assigned to correct second letter of bigram.\n",
    "# Calculate loss.\n",
    "# Use backpropagation to see how much each weight impacts the loss.\n",
    "# Nudge weights in direction that minimizes loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
