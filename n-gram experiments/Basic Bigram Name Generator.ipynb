{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52533478-9b0b-41b8-8b6a-e3cf89d940a7",
   "metadata": {},
   "source": [
    "# Generating Names with Bigrams\n",
    "<!-- Basic Bigram Generator -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f19b2db-e84f-44df-92a5-945800aff081",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1) [Project Overview](#overview)\n",
    "2) [Preparing the Data](#data)\n",
    "3) [Building the Bigram Model](#model)\n",
    "4) [Training the Model](#training)\n",
    "5) [Generating Names](#generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2fae51-6189-4860-8e21-5a18aa9c0cea",
   "metadata": {},
   "source": [
    "<a id=\"overview\"></a>\n",
    "## Project Overview \n",
    "The goal of this project is to create a simple language model that can generate names resembling real ones from its training data.\n",
    "\n",
    "We'll use a neural network trained on character-level bigrams -- that is, pairs of consecutive letters. When the model is given a letter as input, it should predict the probability of each possible next letter. We can then use these probabilities to pick the next letter in a name, then feed that new letter back into the model to continue generating the name, one letter at a time.\n",
    "\n",
    "\n",
    "<!-- There will be two main steps for completing this goal. First, we'll train the model so it learns common bigrams in names. Next, we'll use that trained model to generate new, never-seen-before names. -->\n",
    "\n",
    "\n",
    "<!-- To put that simply, if this model is given a letter, it should learn the probabilities for each letter following that one so that it can produce a likely next letter.  -->\n",
    "\n",
    "<!-- \n",
    "learn which letter might \n",
    "\n",
    "it should find the percent chance that every other \n",
    "\n",
    "this model should learn the likelihood \n",
    "\n",
    "for any letter, the model should output the chance of each other letter following that one in a name.\n",
    "\n",
    "Given a letter, the model should output the probability of each other letter following that one in a name.\n",
    "\n",
    "That is, it will learn the common sequences of two adjacent letters found in names and use that to generate letters that might reasonably follow any given letter in a name. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced6390-bee6-4aaf-adc4-1dab1eaa6e5f",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "## Preparing the Data\n",
    "<!-- We'll be training this model on names so it learns to produce names of its own based on what it learns from the data. First we'll import our dependencies. -->\n",
    "<!-- We'll be storing our data in tensors, so let's start by importing PyTorch.\n",
    "\n",
    "Let's start by importing PyTorch so we can store our data in tensors (and later use  -->\n",
    "<!-- First we'll grab a list of names to use for training. -->\n",
    "We'll start by reading in a list of names we will use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea8ac9df-0b96-4287-9591-4a2de6feba44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the first three names: ['emma', 'olivia', 'ava']\n"
     ]
    }
   ],
   "source": [
    "# Create a list of each name in names.txt\n",
    "names = open(\"names.txt\", \"r\").read().splitlines()\n",
    "print(f\"Displaying the first three names: {names[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2689af61-4b56-4456-a36e-4062f2b94d39",
   "metadata": {},
   "source": [
    "Now that we have our data, let's split it into bigrams so the model can learn common letter patterns in names. We also want it to learn how each name starts and ends, so let's indicate the start and end of a name with a \".\". Below shows an example of the bigrams in the name Emma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a2761a5-07ca-4343-bf02-5b2862fadba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams for emma: [('.', 'e'), ('e', 'm'), ('m', 'm'), ('m', 'a'), ('a', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Break \"emma\" into bigrams, including \".\" for start and end.\n",
    "print(f\"Bigrams for emma: {list(zip('.emma', 'emma.'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532cd13c-5f08-43d5-abf7-e809ae7bd592",
   "metadata": {},
   "source": [
    "<!-- We are going to store these bigrams in tensors to use them in our neural network. -->\n",
    "To use these bigrams in our neural network, we'll organize them into two tensors: X (inputs) and y (labels).\n",
    "<!-- X will contain each first letter within the bigrams. When we train our model, X will be the input.\n",
    "y will contain the second letters within the bigrams, i.e., the solutions to the bigrams started in X. Throughout training, our model should learn to predict the values in y given X. -->\n",
    "- X will contain the first letter of each bigram. These are the inputs we'll feed into the model.\n",
    "- y will contain the second letter of each bigram, i.e., the letter that follows each input letter in X. These are the labels or \"targets\" the model should learn to predict.\n",
    "\n",
    "To start off, let's import Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72dcc032-dbd1-4517-871e-9d48ab9549e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch    # Tensors and backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a42fb6-f8c3-4e48-b3e5-3af0feed7bea",
   "metadata": {},
   "source": [
    "Since PyTorch tensors can't contain character elements, let's assign each letter to an integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67fcb3e7-03f7-46f8-b54b-981b755310f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# For each letter + \".\", add them to dict with an integer value.\n",
    "stoi = {char:int_val for (int_val, char) in enumerate(\".\" + string.ascii_lowercase)}\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ee9ab-b714-4d56-9f94-95f1caa38847",
   "metadata": {},
   "source": [
    "We'll also want a way to go backwards, turning numbers back into letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec968468-135b-48dd-a618-f2ee5b75add6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '.', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "itos = {num: letter for letter, num in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c273ea97-83a4-4cd3-8753-6e54802ec317",
   "metadata": {},
   "source": [
    "Now we are ready to split all the names into bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "353fc34a-6aac-4f6b-b64f-c4d90cac8006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training data has 228146 examples.\n",
      "The first 10 values in X are [0, 5, 13, 13, 1, 0, 15, 12, 9, 22].\n",
      "The first 10 values in y are [5, 13, 13, 1, 0, 15, 12, 9, 22, 9].\n"
     ]
    }
   ],
   "source": [
    "X = [] # First letters in bigrams\n",
    "y = [] # X's bigram pairs.\n",
    "\n",
    "for name in names:\n",
    "    for char1, char2 in zip(\".\" + name, name + \".\"):\n",
    "        # Convert chars to ints so we can later add to tensors.\n",
    "        X.append(stoi[char1])\n",
    "        y.append(stoi[char2])\n",
    "\n",
    "print(f\"Our training data has {len(X)} examples.\")\n",
    "print(f\"The first 10 values in X are {X[:10]}.\")\n",
    "print(f\"The first 10 values in y are {y[:10]}.\")\n",
    "\n",
    "# Convert list to a tensor.\n",
    "X = torch.tensor(X)\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc1c15a-4e0f-48b5-9805-3831086ff727",
   "metadata": {},
   "source": [
    "Before we can use these values in a neural network, we'll need to alter them a bit more. See, neural networks contain weights which get multiplied by each input. Right now our inputs are just numbers from 0 to 26. It wouldn't be very helpful to do math with these inputs, since we want each letter to be treated equally. \"z\" shouldn't have a different multiplier than \"a\", for example.\n",
    "\n",
    "To make sure each input gets treated equally, we can use one-hot encoding. With one-hot encoding, each letter gets represented by an array of length 27 (one for each possible letter including \".\"). This array contains all 0s apart from a single 1 at the index signifying the chosen letter. To show this more visually, let's explore the one-hot encoded version of the letter \"c\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "808bffe7-caa8-4575-88a5-8508992ec230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c's index is 3, so its one-hot encoding looks like tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Get number used to represent c. This will be the index of the 1 after one-hot encoding.\n",
    "c_index = stoi[\"c\"]\n",
    "\n",
    "# Create the one-hot encoded representation of the letter \"c\".\n",
    "c_enc = torch.zeros(27)\n",
    "c_enc[c_index] = 1\n",
    "\n",
    "print(f\"c's index is {c_index}, so its one-hot encoding looks like {c_enc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18171a9-9ec0-4875-a966-5d376093342e",
   "metadata": {},
   "source": [
    "Let's apply this encoding to all the letters in our input data. Rather than encoding them all manually like above, we'll use PyTorch's one_hot() function. \n",
    "\n",
    "<!-- Note that the function makes the datatype of the encodings integers. To use them in our neural network\n",
    "\n",
    "Unlike when I created the encoding myself, however, this will default to making the datatype of each element an integer.  -->\n",
    "\n",
    "<!-- This function doesn't automatically keep the values as floats -->\n",
    "\n",
    "<!-- By default, this function will store the letters  -->\n",
    "<!-- That way, they will all be represented similarly -->\n",
    "\n",
    "<!-- We'll also turn the values into floats while we're at it, as this is necessary so that they can be multiplied by float weights in our neural network. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45a02bce-6d1e-4b42-8deb-811882958e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# One-hot encode X to turn each letter into an array of length 27 (one index for each possible letter including \".\".)\n",
    "X_enc = F.one_hot(X, num_classes = 27)\n",
    "# View encodings for first 5 letters.\n",
    "X_enc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85bfee4-57eb-4438-ad74-bffa495dac25",
   "metadata": {},
   "source": [
    "As you can see, the values here are all integers. To later multiply them by the float weights in our neural network, we'll need to convert those values into floats.\n",
    "\n",
    "<!-- We'll also turn the values into floats while we're at it, as this is necessary so that they can be multiplied by float weights in our neural network. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "088e945f-df59-4662-9795-07e836c53985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want the neural net to produce floats, so the inputs must be floats as well.\n",
    "X_enc = X_enc.float()\n",
    "# The first letter should now be encoded with floats.\n",
    "X_enc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e5fdd6-8340-4902-9662-c7dbfc5455f4",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "## Building the Bigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6215d93b-154e-49a3-ab4a-1867125439f8",
   "metadata": {},
   "source": [
    "<!-- Now that we've finished preparing our data, we're ready to make our neural network. We won't be training it just yet; this section is more about exploring how neural networks work and then showing how we can use it to generate names once it is trained. -->\n",
    "\n",
    "<!-- Our neural network  -->\n",
    "For this project, we'll use a very simple neural network with only a single layer of neurons. \n",
    "\n",
    "<!-- Before creating this model, let's explore how a single neuron works. -->\n",
    "\n",
    "Before we jump into building this layer, it's important to understand how a single neuron works.\n",
    "\n",
    "Since each letter in X now has a length of 27 (due to the encoding), this one neuron will need 27 weights so each element in the inputs can be multiplied by a unique weight. The weights will start off as random numbers from a normal distribution. (After training the model, these numbers should become more meaningful.)\n",
    "\n",
    "\n",
    "<!-- A single neuron wouldn't be a very good network, but we will start with that to understand what exactly is happening in a neural net. -->\n",
    "\n",
    "<!-- Before we get to making our full neural network, we'll start by exploring how a single neuron works. A neuron is  -->\n",
    "<!-- A single neuron wouldn't be a very good network, but it is important to understand how it functions before making  -->\n",
    "\n",
    "<!-- understanding it is -->\n",
    "\n",
    "<!-- we will start with that to understand what exactly is happening in a neural net. -->\n",
    "\n",
    "\n",
    "<!-- for now, we will just explore what a neuron even is, then  -->\n",
    "\n",
    "<!-- this section is more about exploring how neural networks work and then showing how we can use it to generate names once it is trained. -->\n",
    "\n",
    "<!-- for now, we'll just be making the untrained model and walking through how we can use it. -->\n",
    "\n",
    "<!-- we will just be exploring what a neural network is and how it will be used to make predictions.\n",
    "\n",
    "works.\n",
    "\n",
    "TODO: Should we show how the model will make predictions here? -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7fb0bfe-1f20-448c-910d-380bd8c428c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a single neuron with a 27 weights:\n",
      "tensor([[ 1.9269],\n",
      "        [ 1.4873],\n",
      "        [ 0.9007],\n",
      "        [-2.1055],\n",
      "        [ 0.6784],\n",
      "        [-1.2345],\n",
      "        [-0.0431],\n",
      "        [-1.6047],\n",
      "        [-0.7521],\n",
      "        [ 1.6487],\n",
      "        [-0.3925],\n",
      "        [ 0.2415],\n",
      "        [-1.1109],\n",
      "        [ 0.0915],\n",
      "        [-2.3169],\n",
      "        [-0.2168],\n",
      "        [-1.3847],\n",
      "        [-0.8712],\n",
      "        [-0.2234],\n",
      "        [-0.6216],\n",
      "        [-0.5920],\n",
      "        [-0.0631],\n",
      "        [-0.8286],\n",
      "        [ 0.3309],\n",
      "        [-1.5576],\n",
      "        [ 0.9956],\n",
      "        [-0.8798]])\n"
     ]
    }
   ],
   "source": [
    "# Weights start off random, so let's use a generator.\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "\n",
    "# A single neuron with one weight for each element in our input.\n",
    "W = torch.randn((27, 1), generator=gen)\n",
    "print(\"Here is a single neuron with a 27 weights:\")\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b5d0b-e7b8-44ea-920f-34b382eca936",
   "metadata": {},
   "source": [
    "<!-- TODO: Finish rewording this. (And maybe change W's name since it isn't a full network? But it is still weights, so maybe keep it?) -->\n",
    "\n",
    "<!-- To use a neural network, we multiply  -->\n",
    "\n",
    "<!-- Now that we have a neuron, let's try using it with our input to how it works. To use a neural network, we multiply each input by the weights within its neurons. -->\n",
    "<!-- To use a neural network, we multiply each input by the weights within its neurons. Here we only have one neuron, so \n",
    "\n",
    "We can use a neural network by multiplying each input \n",
    "\n",
    "Let's try using this neuron! As mentioned earlier, we use this network by multiplying each input by the weights of this neuron.\n",
    " -->\n",
    "Now we can use this neuron by multiplying our inputs with it.\n",
    "\n",
    "\n",
    "When the input is a one-hot encoded vector, the dot product multiplies all weights by 0 except for the one weight at the position where the input has a 1. Thus, each one-hot encoded input simply selects the corresponding weight from the neuron and ignores the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9a6fe85-6afa-4ee7-9c92-623e1cde8602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_enc has a 1 at index 3, so the dot product should select the neuron's weight at index 3.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-2.1055])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"c_enc has a 1 at index 3, so the dot product should select the neuron's weight at index 3.\")\n",
    "c_enc @ W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660294e0-4975-4045-98b4-a9464b6a097a",
   "metadata": {},
   "source": [
    "<!-- Now let's think for a moment about the ideal behavior of our neural network. As mentioned in the overview, we'd like our model to take a letter as input and determine the probability for each possible next letter. Since there are 27 possible options for next letters, our model should produce 27 outputs for each input letter (where each output indicates the probability of the letter at that index coming next). -->\n",
    "\n",
    "As described earlier, we want our model to take a letter as input and output the probability of each possible next letter. Since there are 27 possible next letters, the model should produce a vector of 27 probabilities, one for each letter.\n",
    "\n",
    "We've seen that a single neuron produces one output. Thus, to produce 27 outputs, we'll need a layer of 27 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a5caf1e-96f7-4813-b82a-a86550bd4311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting generator so cell always gives same numbers instead of generating the next numbers.\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "\n",
    "# Each column in the weights matrix represents another neuron in the layer.\n",
    "W = torch.randn((27, 27), generator = gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01f61b3-0c77-4414-8478-447c26b5bf76",
   "metadata": {},
   "source": [
    "Now W (our weights) is a 27x27 matrix of random values from a normal distribution. Each column represents a single neuron in the layer. \n",
    "\n",
    "Before we explored how one-hot encoding selected a single value from the neuron. That's still true, but now that there are 27 neurons, it will select the value in that same row for each of the neurons. So for example, an input of c (index 3) dot producted with the layer would result in a vector of all the weights at the row with index 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f033286-894f-4c58-a3a2-3498b0dc1500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3387, -1.3407, -0.5854,  0.5362,  0.5246,  1.1412,  0.0516,  0.7440,\n",
       "        -0.4816, -1.0495,  0.6039, -1.7223, -0.8278,  1.3347,  0.4835, -2.5095,\n",
       "         0.4880,  0.7846,  0.0286,  0.6408,  0.5832,  1.0669, -0.4502, -0.1853,\n",
       "         0.7528,  0.4048,  0.1785])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the third weight from each neuron.\n",
    "c_output = c_enc @ W\n",
    "c_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da32450-2be9-4486-a1d5-e6424c5e64ca",
   "metadata": {},
   "source": [
    "Now we have a model that can take a letter as input and produce 27 outputs, one for each possible next letter. \n",
    "\n",
    "You may notice that the numbers in our output don't look very much like probabilities just yet. At this stage, the outputs are just raw scores (also called \"logits\"), not probabilities. When we use this model during training and sampling, we'll need to normalize these outputs to turn them into probabilities. \n",
    "\n",
    "<!-- Below shows each letter and its associated logit. -->\n",
    "\n",
    "<!-- To make it more clear how each logit is associated with a letter,  -->\n",
    "\n",
    "<!-- To make this more clear, below we can see how each value in this output vector corresponds to a specific letter. -->\n",
    "\n",
    "<!-- Each value in this output vector corresponds to a specific letter and can eventually be used to find their probability.  -->\n",
    "\n",
    "<!-- Let's match up each possible next letter with the value -->\n",
    "\n",
    "<!-- Ideally, we'd like these values to represent the probabilities of each letter coming after \"c\". However, so far, the outputs are just raw scores (called \"logits\"), not probabilities.  -->\n",
    "\n",
    "<!-- Logits must be converted into \n",
    "\n",
    "\n",
    "Now we have a model that outputs 27 values for each input letter, but these values aren't yet probabilities. Instead, what we have are logits\n",
    "\n",
    "So far what we have are \n",
    "\n",
    "\n",
    "Now we have a model that can take a letter as input and produce 27 outputs, one for each possible next letter. These values aren't probabilities just yet. So far, we only \n",
    "\n",
    "Now we have a model that can take a letter as input and produce 27 outputs, one for each possible next letter. Each value in this output vector corresponds to a specific letter and can eventually be used to find their probability. \n",
    "\n",
    "\n",
    "After training the model, these numbers can be used to find the probability that each \n",
    "\n",
    "These aren't yet the probabilities of \n",
    "\n",
    "Ideally, we'd like these values to represent the probabilities of each letter coming after \"c\". However, so far, the outputs are just raw scores (also called \"logits\"), not probabilities.\n",
    "\n",
    "\n",
    "Ideally, we'd like these values to represent the probabilities of each letter coming after \"c\". Of course, our model isn't trained yet, so we can't expect the probabilities to be correct just yet. But if we take a look at the \n",
    "\n",
    "these numbers should still be random. But \n",
    "\n",
    "\n",
    "\n",
    "But let's take a closer look at these values before going further.\n",
    "\n",
    "Ideally, we'd like these values to represent the probabilities of each letter coming after \"c\". However, so far, the outputs are just raw scores (also called \"logits\"), not probabilities.\n",
    "\n",
    "Just to make it more clear, let's \n",
    "\n",
    "<!-- Ideally, we'd like each of these outputs to represent the probability that the letter at that index comes after \"c\". To make this clearer, let's match up each output with the letter it represents. -->\n",
    "\n",
    "<!-- let's match up each possible next letter with the value  -->\n",
    "\n",
    "<!-- we can match each of these values with the letter it coincides with. -->\n",
    "\n",
    "<!-- However, you may notice that the numbers in our output don't look very much like probabilities just yet. --> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "911ffc48-a3c9-4915-94b6-fb39cca04b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our untrained model's current raw scores for each letter following 'c':\n",
      ".: -0.3387\n",
      "a: -1.3407\n",
      "b: -0.5854\n",
      "c: 0.5362\n",
      "d: 0.5246\n",
      "e: 1.1412\n",
      "f: 0.0516\n",
      "g: 0.7440\n",
      "h: -0.4816\n",
      "i: -1.0495\n",
      "j: 0.6039\n",
      "k: -1.7223\n",
      "l: -0.8278\n",
      "m: 1.3347\n",
      "n: 0.4835\n",
      "o: -2.5095\n",
      "p: 0.4880\n",
      "q: 0.7846\n",
      "r: 0.0286\n",
      "s: 0.6408\n",
      "t: 0.5832\n",
      "u: 1.0669\n",
      "v: -0.4502\n",
      "w: -0.1853\n",
      "x: 0.7528\n",
      "y: 0.4048\n",
      "z: 0.1785\n"
     ]
    }
   ],
   "source": [
    "print(\"Our untrained model's current raw scores for each letter following 'c':\")\n",
    "for letter, logit in zip(stoi.keys(), c_output):\n",
    "    print(f\"{letter}: {logit:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8318672-2de8-487e-90eb-43d3882c6c18",
   "metadata": {},
   "source": [
    "<!-- Of course, our data (stored in X_enc) contains much more than just a single letter. If our input contains many letters (such as all the letters stored in X), the output will still produce a vector for each letter in the input, but each of these vectors will be combined as rows of a matrix. Thus, if we send the network all of X as input, the resulting output will be a matrix with a row of probabilities for each of the 228146 examples letter in X. -->\n",
    "\n",
    "Of course, our data (stored in `X_enc`) contains much more than just a single letter of input. When we input many letters at once (for example, all the letters in `X`), the network will output a vector of logits for each input letter. These vectors are stacked together as rows in a matrix. Thus, if we pass all of `X` as input, the output will be a matrix where each row contains the logits for the next letter, corresponding to each of the 228146 input letters in `X`.\n",
    "\n",
    "<!-- , but multiplying `X_enc @ W` will result in a vector just like above for each letter in X. The only difference is that instead of the network outputting a single vector,  -->\n",
    "\n",
    "<!-- . When we use this neural network on all the letters in X, we'll get a matrix where each row is a vector just like above  -->\n",
    "\n",
    "<!-- , but multiplying `X_enc @ W` will result in a vector just like above for each letter in X. These outputs  -->\n",
    "<!-- Each of these vectors will be stored as a row in a final output matrix. -->\n",
    "\n",
    "<!-- Now that we have a model that can produce 27 outputs for each input, we're on the right track. However, you may notice that the numbers in our output don't look very much like probabilities just yet. -->\n",
    "\n",
    "<!-- Now let's think about these output vectors a bit. We want each output to tell us the probability of each other letter following the input letter. Right now, the numbers in our output don't look very much like probabilities. For starters, some of them are negative. We can fix that using exponentiation, which shifts numbers over so that negative numbers end up as small decimal values and positive numbers end up larger than 1. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97ce7428-347b-4a32-91c4-dd16c0ff89bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431, -1.6047,\n",
       "         -0.7521,  1.6487, -0.3925, -1.4036, -0.7279, -0.5594, -0.7688,  0.7624,\n",
       "          1.6423, -0.1596, -0.4974,  0.4396, -0.7581,  1.0783,  0.8008,  1.6806,\n",
       "          1.2791,  1.2964,  0.6105],\n",
       "        [ 0.5760,  1.1415,  0.0186, -1.8058,  0.9254, -0.3753,  1.0331, -0.6867,\n",
       "          0.6368, -0.9727,  0.9585,  1.6192,  1.4506,  0.2695, -0.2104, -0.7328,\n",
       "          0.1043,  0.3488,  0.9676, -0.4657,  1.6048, -2.4801, -0.4175, -1.1955,\n",
       "          0.8123, -1.9006,  0.2286]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the probability vectors for the first two letters in X.\n",
    "X_enc[:2] @ W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90b003e-a66e-4b55-af59-95bfebee1f2e",
   "metadata": {},
   "source": [
    "___________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2047b06-4617-4ef0-9422-dbd025f510b1",
   "metadata": {},
   "source": [
    "<!-- Now that we have a model that can produce 27 output probabilities for each letter, we're on the right track. However, y -->\n",
    "\n",
    "You may notice that the numbers in our output don't look very much like probabilities just yet. At this stage, the outputs are just raw scores (also called \"logits\"), not probabilities. When we use this model during training and sampling, we'll need to normalize these outputs to turn them into probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa7475b-3a10-465e-ae2c-46f7cdbcd692",
   "metadata": {},
   "source": [
    "<!-- Ideally, we'd like these values to represent the probabilities of each letter coming after \"c\". However, so far, the outputs are just raw scores (also called \"logits\"), not probabilities. -->\n",
    "\n",
    "<!-- Some of these supposed probabilities are negative, which doesn't make much sense.  -->\n",
    "\n",
    "Now that we have a model that can produce 27 output probabilities for each letter, we're on the right track. However, you may notice that the numbers in our output don't look very much like probabilities just yet. Right now, what we have are logits: the raw output before we do any normalization. For them to be probabilities, they'd each need to be a number between 0 and 1, and their sum should add up to 1 (for 100%). To convert the model's raw scores (also called logits) into probabilities, we'll need to normalize the output. This can be done with the softmax function. The softmax function works as follows.\n",
    "\n",
    "First, we exponentiate each of the outputs. This converts all values to positive numbers while perserving their order.\n",
    "\n",
    "<!-- This fixes the issue of negative logits, as exponentiating makes all the digits positive in such a way that keeps values that were negative smaller than values that were positive. -->\n",
    "<!-- \n",
    ", like so:\n",
    "\n",
    "We can fix the issue of negatives by exponentiating the values, making all the digits positive while keeping values that were negative smaller than values that were positive. Then we can turn this into a probability by summing up each row and setting the weights for that row equal to their fraction of that sum. This process is called the softmax function. -->\n",
    "\n",
    "<!-- (indicating 100% probability). -->\n",
    "\n",
    "<!-- Right now, what we have are logits: the raw output before we do any normalization. When we use this model during training and sampling, we'll need to normalize these outputs to turn them into probabilities. We can do this using the softmax function.  -->\n",
    "\n",
    "<!-- To understand the softmax function, let's take another look at the output we saw when we used \"c\" as our input. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "80b2a112-3706-41cc-af90-5a69fc4a5e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7127, 0.2617, 0.5569, 1.7095, 1.6898, 3.1305, 1.0530, 2.1042, 0.6178,\n",
       "        0.3501, 1.8292, 0.1787, 0.4370, 3.7989, 1.6218, 0.0813, 1.6291, 2.1915,\n",
       "        1.0291, 1.8979, 1.7918, 2.9064, 0.6375, 0.8309, 2.1228, 1.4989, 1.1954])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exponentiate the logits so that none are negative.\n",
    "c_pos = c_output.exp()\n",
    "c_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de1f8e0-50b2-4449-8f64-42f0a279772f",
   "metadata": {},
   "source": [
    "Next, we turn this into a probability by summing \n",
    "\n",
    "up each row and setting the weights for that row equal to their fraction of that sum. This process is called the softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eb5532-ac41-4552-84f3-0aa59f91907c",
   "metadata": {},
   "source": [
    "____________\n",
    "Explain softmax, then show what happens with multiple inputs (shown below), then say how we'd need to use softmax for all of those."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a824013-7566-478d-9dea-d750a9d0e2e3",
   "metadata": {},
   "source": [
    "Now that we have a model that can produce 27 output probabilities for each letter, we're on the right track. However, you may notice that the numbers in our output don't look very much like probabilities just yet. Right now, what we have are logits: the raw output before we do any normalization. When we use this model during training and sampling, we'll need to normalize these outputs to turn them into probabilities. We can do this using the softmax function. To understand the softmax function, let's take another look at the output we saw when we used \"c\" as our input.\n",
    "<!-- \n",
    "The softmax function is pretty simple. \n",
    "\n",
    "\n",
    "First, you may have noticed that many of the values above are negative. Probabilities can't be negative. An easy fix to this is to exponentiate them. -->\n",
    "\n",
    "<!-- So for now, let's explore how that normalization works. -->\n",
    "\n",
    "<!-- We can do this using the softmax function.  -->\n",
    "\n",
    "<!-- , which will be explored in futher detail in the following section. -->\n",
    "<!-- \n",
    "To understand the softmax function, let's examine what we get when our input to the neural network is the letter \"c\".\n",
    "\n",
    "We can explore the softmax function with the output of \n",
    "\n",
    "\n",
    "\n",
    "The softmax function works like so:\n",
    "\n",
    "First, we need  -->\n",
    "\n",
    "\n",
    "<!-- TODO: FINISH ABOVE! -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7d0a190-b24c-420d-9630-4a8874c5cb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3387, -1.3407, -0.5854,  0.5362,  0.5246,  1.1412,  0.0516,  0.7440,\n",
       "        -0.4816, -1.0495,  0.6039, -1.7223, -0.8278,  1.3347,  0.4835, -2.5095,\n",
       "         0.4880,  0.7846,  0.0286,  0.6408,  0.5832,  1.0669, -0.4502, -0.1853,\n",
       "         0.7528,  0.4048,  0.1785])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-examine what happens when we input \"c\" into our network.\n",
    "c_enc @ W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432fbd5d-46d9-4392-a871-02453a403380",
   "metadata": {},
   "source": [
    "Remember, we'd like each value in this output tensor to show us the probability of a letter following \"c\". Right now, the probabilities are like so:\n",
    "\n",
    "TODO, \n",
    "\n",
    "<!-- See how many of those numbers are negative? A negative probability doesn't mean much, and so the first step of the softmax function is to exponentiate the values. This \n",
    "\n",
    ", so we'll\n",
    "\n",
    "They are negative and don't sum up to 1 (100%). We can fit the issue of negatives by exponentiating the values, making all the digits positive while keeping values that were negative smaller than values that were positive. Then we can turn this into a probability by summing up each row and setting the weights for that row equal to their fraction of that sum. This process is called the softmax function. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e029a1ee-61b8-4412-a794-09527e2d6bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', tensor(-0.3387)),\n",
       " ('a', tensor(-1.3407)),\n",
       " ('b', tensor(-0.5854)),\n",
       " ('c', tensor(0.5362)),\n",
       " ('d', tensor(0.5246)),\n",
       " ('e', tensor(1.1412)),\n",
       " ('f', tensor(0.0516)),\n",
       " ('g', tensor(0.7440)),\n",
       " ('h', tensor(-0.4816)),\n",
       " ('i', tensor(-1.0495)),\n",
       " ('j', tensor(0.6039)),\n",
       " ('k', tensor(-1.7223)),\n",
       " ('l', tensor(-0.8278)),\n",
       " ('m', tensor(1.3347)),\n",
       " ('n', tensor(0.4835)),\n",
       " ('o', tensor(-2.5095)),\n",
       " ('p', tensor(0.4880)),\n",
       " ('q', tensor(0.7846)),\n",
       " ('r', tensor(0.0286)),\n",
       " ('s', tensor(0.6408)),\n",
       " ('t', tensor(0.5832)),\n",
       " ('u', tensor(1.0669)),\n",
       " ('v', tensor(-0.4502)),\n",
       " ('w', tensor(-0.1853)),\n",
       " ('x', tensor(0.7528)),\n",
       " ('y', tensor(0.4048)),\n",
       " ('z', tensor(0.1785))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(stoi.keys(), c_enc @ W))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedc9018-019a-42ea-b527-1c075cbfc214",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id=\"training\"></a>\n",
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709da763-c837-4365-9bf3-7e9f103c0269",
   "metadata": {},
   "source": [
    "Now that we have a model that can produce 27 outputs for each input, we're on the right track. However, you may notice that the numbers in our output don't look very much like probabilities just yet. We'll need to fix that when training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d235a54-5ed6-4025-b657-6d331c980d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train:\n",
    "# Use input. Check percent it assigned to label. Make it higher."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
