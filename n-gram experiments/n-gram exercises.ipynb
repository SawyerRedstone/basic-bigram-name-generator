{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67d41879-2147-47a3-b10e-517a665d14f3",
   "metadata": {},
   "source": [
    "# N-Gram Language Model Exercises\n",
    "Inspired by Andrej Karpathy's first Makemore video. Here I'll try to reproduce what I learned from scratch and add additional features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e798106-4684-4803-a2e8-6215e25e0b71",
   "metadata": {},
   "source": [
    "We'll start by making a bigram language model. We'll train it on names so it learns to produce names of its own based on what it learned from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01964050-2833-478b-ad93-786d50a5ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                       # Tensors and backpropagation.\n",
    "import matplotlib.pyplot as plt    # Graphing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23da100d-2c5b-47bc-978e-784256ab5774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the first three names: ['emma', 'olivia', 'ava']\n"
     ]
    }
   ],
   "source": [
    "# Create a list of each name in names.txt\n",
    "names = open(\"names.txt\", \"r\").read().splitlines()\n",
    "print(f\"Displaying the first three names: {names[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee68b090-608d-4361-b2bb-9eccf95baf57",
   "metadata": {},
   "source": [
    "Now that we have our data, let's split it into bigrams so the model can learn common letter patterns in names. We also want it to learn how each name starts and ends, so let's indicate the start and end of a name with a \".\". Below shows an example of the bigrams in the name Emma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "626b587d-a305-430b-beb0-56f9ea43f841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams for emma: [('.', 'e'), ('e', 'm'), ('m', 'm'), ('m', 'a'), ('a', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Break \"emma\" into bigrams, including \".\" for start and end.\n",
    "print(f\"Bigrams for emma: {list(zip('.emma', 'emma.'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9862de-7eb5-4725-b142-d252dd929ca5",
   "metadata": {},
   "source": [
    "We want this model to take a character as input and predict which character comes next. For that, we want two tensors.\n",
    "\n",
    "X (the input) should contain the first half of the bigrams.\n",
    "\n",
    "y (the label) should contain the solutions, AKA the letter that comes next.\n",
    "\n",
    "Since PyTorch tensors can't contain character elements, let's first assign each letter to an integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb4007fb-5a85-46bd-b671-aef040f0b5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# For each letter + \".\", add them to dict with an integer value.\n",
    "stoi = {char:int_val for (int_val, char) in enumerate(\".\" + string.ascii_lowercase)}\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2815c0af-01ef-4f8b-852e-1dc54c8b1af0",
   "metadata": {},
   "source": [
    "<!-- Now we'll get the bigrams for all the names and split them into two tensors, X and y. -->\n",
    "<!-- Now we are ready to get bigrams for all the names  and put them into tensors X and y. -->\n",
    "Now we are ready to make our bigrams for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a97fd295-280f-43ca-a442-5de4ef35221a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 values in X are [0, 5, 13, 13, 1, 0, 15, 12, 9, 22].\n",
      "The first 10 values in y are [5, 13, 13, 1, 0, 15, 12, 9, 22, 9].\n"
     ]
    }
   ],
   "source": [
    "X = [] # First letters in bigrams\n",
    "y = [] # X's bigram pairs.\n",
    "\n",
    "for name in names:\n",
    "    for char1, char2 in zip(\".\" + name, name + \".\"):\n",
    "        # Convert chars to ints so we can later add to tensors.\n",
    "        X.append(stoi[char1])\n",
    "        y.append(stoi[char2])\n",
    "\n",
    "\n",
    "print(f\"The first 10 values in X are {X[:10]}.\")\n",
    "print(f\"The first 10 values in y are {y[:10]}.\")\n",
    "# Convert list to a tensor.\n",
    "X = torch.tensor(X)\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33e11d2-062c-42b0-b36d-b1987fc6c391",
   "metadata": {},
   "source": [
    "Now we are ready to make our neural network. Neural networks have weights that get multiplied by each input. But right now our inputs are just numbers from 0 to 26. It wouldn't be very helpful to do math with these inputs, since we want each letter to be treated equally. `z` shouldn't have a different multiplier than `a`, for example.\n",
    "\n",
    "To make sure each input gets treated equally, we can use one-hot encoding. With one-hot encoding, each letter gets represented by an array of length 27 (one for each possible letter including \".\"). This array contains all 0s apart from a single 1 at the index signifying the chosen letter.\n",
    "<!-- This converts each input into an array filled mostly with 0s, with a single 1 in the index that signifies out letter. -->\n",
    "\n",
    "<!-- or else letters represented by large numbers (like `z`) will end up having a larger . `z` is not worth more than `a` even though it is represented by a larger integer. -->\n",
    "<!-- , since they don't have any mathematical meaning. `z` is not worth more than `a` even though it is represented by a larger integer. -->\n",
    "<!-- The neural network will have weights that get multiplied by each of our  -->\n",
    "<!-- A neural network contains weights to multiply with its  -->\n",
    "\n",
    "<!-- multiplies its weights by each input. However, right now -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0ef0388-62df-441f-810d-4c8d66825c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c's index is 3, so its one-hot encoding looks like tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "c_index = stoi[\"c\"]\n",
    "\n",
    "# Create the one-hot encoded representation of the letter \"c\".\n",
    "c_enc = torch.zeros(27)\n",
    "c_enc[c_index] = 1\n",
    "\n",
    "print(f\"c's index is {c_index}, so its one-hot encoding looks like {c_enc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ee4540-42f8-4a47-bd52-40c125d714f9",
   "metadata": {},
   "source": [
    "Let's apply this encoding to all the letters in our input data. We'll also turn the values into floats while we're at it, as this is necessary so that they can be multiplied by float weights in our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f173b44-4bb1-485e-8aee-3d49fd3945e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# One-hot encode X to turn each letter into an array of length 27 (one index for each possible letter including \".\".)\n",
    "X_enc = F.one_hot(X, num_classes = 27)\n",
    "# We want the neural net to produce floats, so the inputs must be floats as well.\n",
    "X_enc = X_enc.float()\n",
    "# View first 5 encodings.\n",
    "print(X_enc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc787b0-a359-475e-aa61-4c2b49888e96",
   "metadata": {},
   "source": [
    "Now we can start building our network. A single neuron wouldn't be a very good network, but we will start with that to understand what exactly is happening in a neural net. \n",
    "\n",
    "Since each input now has a length of 27 (due to the encoding), this one neuron will actually need 27 weights so each element of the inputs can be multiplied by a unique weight. The weights will start off as random numbers from a normal distribution. (After training the model, these numbers should become more meaningful.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aada1ff3-53ff-4a9c-8f28-3721315ec2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9269],\n",
       "        [ 1.4873],\n",
       "        [ 0.9007],\n",
       "        [-2.1055],\n",
       "        [ 0.6784],\n",
       "        [-1.2345],\n",
       "        [-0.0431],\n",
       "        [-1.6047],\n",
       "        [-0.7521],\n",
       "        [ 1.6487],\n",
       "        [-0.3925],\n",
       "        [ 0.2415],\n",
       "        [-1.1109],\n",
       "        [ 0.0915],\n",
       "        [-2.3169],\n",
       "        [-0.2168],\n",
       "        [-1.3847],\n",
       "        [-0.8712],\n",
       "        [-0.2234],\n",
       "        [-0.6216],\n",
       "        [-0.5920],\n",
       "        [-0.0631],\n",
       "        [-0.8286],\n",
       "        [ 0.3309],\n",
       "        [-1.5576],\n",
       "        [ 0.9956],\n",
       "        [-0.8798]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weights start off random, so let's use a generator.\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "\n",
    "# A single neuron with one weight for each element in our input.\n",
    "W = torch.randn((27, 1), generator=gen)    # Weights are random nums from normal distribution.\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1589fe55-5a2e-4178-8cdf-0ee0584ffd2c",
   "metadata": {},
   "source": [
    "Let's try using this (very basic) network! As mentioned earlier, we use this network by multiplying each input by the weights of this neuron. \n",
    "\n",
    "When the input is a one-hot encoded vector, the dot product multiplies all weights by 0 except for the one weight at the position where the input has a 1. So, each one-hot encoded input simply selects the corresponding weight from the neuron and ignores the rest. \n",
    "<!-- To demonstrate, let's look at  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89585ab7-8f0c-40f2-8211-5f3dca6df351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.1055])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c_enc has a 1 at index 3, so the dot product should select the neuron's weight at index 3.\n",
    "c_enc @ W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91068433-e69b-4587-93eb-34204a9b287e",
   "metadata": {},
   "source": [
    "Let's think for a moment about the ideal behavior of our neural network. We'd like our model to take a letter as input and determine the probability that each other letter would follow it. Since there are 27 possible options for next letters, we'd like our model to produce 27 outputs. Since each neuron only produces one output, we'll need 27 of them. We'll combine all these neurons in a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb7c9d8b-aaac-4a73-8ff7-d18f1a582bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting generator so cell always gives same numbers instead of generating the next numbers.\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "\n",
    "# Each column in the weights matrix represents another neuron in the layer.\n",
    "W = torch.randn((27, 27), generator = gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6404324-0d74-44bd-a98a-221b08cf6c58",
   "metadata": {},
   "source": [
    "Now W (our weights) is a 27x27 matrix of random values from a normal distribution. Each column represents a single neuron in the layer. Before we explored how one-hot encoding selected a single value from the neuron. That's still true, but now that there are 27 neurons, it will select the value in that same row for each of the neurons. So for example, an input of c (index 3) dot producted with the layer would result in a vector of all the weights at the row with index 3.\n",
    "\n",
    "<!-- , resulting in a vector of each value selected per neuron. -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94f33c0a-35c3-4b5f-b7ac-52a4e80b8834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3387, -1.3407, -0.5854,  0.5362,  0.5246,  1.1412,  0.0516,  0.7440,\n",
       "        -0.4816, -1.0495,  0.6039, -1.7223, -0.8278,  1.3347,  0.4835, -2.5095,\n",
       "         0.4880,  0.7846,  0.0286,  0.6408,  0.5832,  1.0669, -0.4502, -0.1853,\n",
       "         0.7528,  0.4048,  0.1785])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the third weight from each neuron.\n",
    "c_enc @ W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38a1097-75d7-4df0-8447-24b7ac4d21c6",
   "metadata": {},
   "source": [
    "<!-- We are now ready to use this newly updated model! Instead of only using this model with the letter \"c\", we'll use it on all the letters stored in X.  -->\n",
    "<!-- Now that we thoroughly understand how this layer of neurons works, let's use them on all our data. -->\n",
    "\n",
    "<!-- TODO: CONTINUE FROM HERE! -->\n",
    "Of course, our data (stored in X_enc) contains much more than just a single letter, but multiplying `X_enc @ W` will result in a vector just like above for each letter in X. Now let's think about these output vectors a bit. We want each output to tell us the probability of each other letter following the input letter. Right now, the numbers in our output don't look very much like probabilities. For starters, some of them are negative. We can fix that using exponentiation, which shifts numbers over so that negative numbers end up as small decimal values and positive numbers end up larger than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e349816f-e322-4825-b9da-50caeff80e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7127, 0.2617, 0.5569, 1.7095, 1.6898, 3.1305, 1.0530, 2.1042, 0.6178,\n",
       "        0.3501, 1.8292, 0.1787, 0.4370, 3.7989, 1.6218, 0.0813, 1.6291, 2.1915,\n",
       "        1.0291, 1.8979, 1.7918, 2.9064, 0.6375, 0.8309, 2.1228, 1.4989, 1.1954])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing the output with the \"c\" example from earlier.\n",
    "pos_output = (c_enc @ W).exp()\n",
    "pos_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5403be-2069-4b26-991a-995435167022",
   "metadata": {},
   "source": [
    "This is better, but since each of the 27 values in the output should show the percent chance of the letter in that index appearing next, we need all those values to sum up to 1, meaning 100%. After all, in total, there is 100% chance that *some* letter will follow this one. We can fix this by summing up all the values in the resulting vector and turning each element into a ratio of that sum.\n",
    "\n",
    "<!-- changing each value into the fraction of  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "40ee1abc-521d-46fb-a119-8e2f9b6c70b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0188, 0.0069, 0.0147, 0.0451, 0.0446, 0.0827, 0.0278, 0.0556, 0.0163,\n",
       "        0.0092, 0.0483, 0.0047, 0.0115, 0.1003, 0.0428, 0.0021, 0.0430, 0.0579,\n",
       "        0.0272, 0.0501, 0.0473, 0.0768, 0.0168, 0.0219, 0.0561, 0.0396, 0.0316])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the output for \"c\" into probabilities of each next letter occurring.\n",
    "probs = pos_output / sum(pos_output)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "43c2649d-d66e-4ee6-9320-3d281c608c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the probabilities should add up to 1.\n",
    "sum(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb5427d-562d-44e0-b9e0-e77fbf9fd3f0",
   "metadata": {},
   "source": [
    "Now we are ready to use this neural network on the rest of our dataset.\n",
    "<!-- apply this to the rest of the dataset -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9acc25d7-38a8-45a6-8208-5a6b62d83f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1230, 0.0793, 0.0441,  ..., 0.0644, 0.0655, 0.0330],\n",
       "        [0.0396, 0.0698, 0.0227,  ..., 0.0502, 0.0033, 0.0280],\n",
       "        [0.0123, 0.0078, 0.0250,  ..., 0.0340, 0.0149, 0.0053],\n",
       "        ...,\n",
       "        [0.0044, 0.0085, 0.0378,  ..., 0.0071, 0.0041, 0.2970],\n",
       "        [0.0843, 0.0516, 0.0303,  ..., 0.0050, 0.0154, 0.0347],\n",
       "        [0.0435, 0.0354, 0.0071,  ..., 0.2319, 0.0060, 0.0317]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the original output for our dataset values before applying any functions.\n",
    "logits = X_enc @ W\n",
    "# Exponentiate to make the outputs positive.\n",
    "pos_output = logits.exp()\n",
    "# Turn the outputs into probabilities of each next letter occurring.\n",
    "probs = pos_output / pos_output.sum(dim=1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0e5e0c19-d45f-446a-9c39-8bf71bce2bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 27])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we should have probabilities for each letter in our dataset.\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1127114c-d8e0-477c-9e68-06be1150f04f",
   "metadata": {},
   "source": [
    "Now we need a way for our model to make predictions. So far it should produce probabilities, but we don't want the results to be deterministic and always give us the letter with the highest probability. At best, that wouldn't be very creative, and at worst that may cause an infinite loop. Instead, we'll use multinomial sampling to select letters randomly based on their weights. \n",
    "\n",
    "<!-- Torch's multinomial method can take a tensor of probabilities and select -->\n",
    "\n",
    "<!-- takes a tensor of probabilities and returns a number of indices randomly chosen based on the probabilities in the input tensor. -->\n",
    "\n",
    "<!-- samples an index from it based on  -->\n",
    "<!-- Multinomial sampling takes a tensor -->\n",
    "\n",
    "<!-- sample from the probabilities using -->\n",
    "\n",
    "<!-- Now that our model is set up, we're ready to train it. For that, we'll need a loss function to minimize. Here we'll use negative log likelihood.  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0c02689b-ee41-408d-8548-870b9d0e2831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen = torch.Generator().manual_seed(42)\n",
    "\n",
    "# # Get probabilities of each letter following \".\".\n",
    "# curr_probs = probs[0]\n",
    "\n",
    "# # Randomly pick an index based on the weights within those indices.\n",
    "# torch.multinomial(curr_probs, num_samples=1, replacement=True, generator=gen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
