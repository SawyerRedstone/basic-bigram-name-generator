{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67d41879-2147-47a3-b10e-517a665d14f3",
   "metadata": {},
   "source": [
    "# N-Gram Language Model Exercises\n",
    "Inspired by Andrej Karpathy's first Makemore video. Here I'll try to reproduce what I learned from scratch and add additional features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e798106-4684-4803-a2e8-6215e25e0b71",
   "metadata": {},
   "source": [
    "We'll start by making a bigram language model. We'll train it on names so it learns to produce names of its own based on what it learned from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01964050-2833-478b-ad93-786d50a5ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch adds tensors and backpropagation.\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23da100d-2c5b-47bc-978e-784256ab5774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the first three names: ['emma', 'olivia', 'ava']\n"
     ]
    }
   ],
   "source": [
    "# Create a list of each name in names.txt\n",
    "names = open(\"names.txt\", \"r\").read().splitlines()\n",
    "print(f\"Displaying the first three names: {names[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee68b090-608d-4361-b2bb-9eccf95baf57",
   "metadata": {},
   "source": [
    "Now that we have our data, let's split it into bigrams so the model can learn common letter patterns in names. We also want it to learn how each name starts and ends, so let's indicate the start and end of a name with a \".\". Below shows an example of the bigrams in the name Emma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "626b587d-a305-430b-beb0-56f9ea43f841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams for emma: [('.', 'e'), ('e', 'm'), ('m', 'm'), ('m', 'a'), ('a', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Break \"emma\" into bigrams, including \".\" for start and end.\n",
    "print(f\"Bigrams for emma: {list(zip('.emma', 'emma.'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9862de-7eb5-4725-b142-d252dd929ca5",
   "metadata": {},
   "source": [
    "We want this model to take a character as input and predict which character comes next. For that, we want two tensors.\n",
    "\n",
    "X (the input) should contain the first half of the bigrams.\n",
    "\n",
    "y (the label) should contain the solutions, AKA the letter that comes next.\n",
    "\n",
    "Since PyTorch tensors can't contain character elements, let's first assign each letter to an integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb4007fb-5a85-46bd-b671-aef040f0b5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# For each letter + \".\", add them to dict with an integer value.\n",
    "stoi = {char:int_val for (int_val, char) in enumerate(\".\" + string.ascii_lowercase)}\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2815c0af-01ef-4f8b-852e-1dc54c8b1af0",
   "metadata": {},
   "source": [
    "<!-- Now we'll get the bigrams for all the names and split them into two tensors, X and y. -->\n",
    "<!-- Now we are ready to get bigrams for all the names  and put them into tensors X and y. -->\n",
    "Now we are ready to make our bigrams for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a97fd295-280f-43ca-a442-5de4ef35221a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 10 values in X are [0, 5, 13, 13, 1, 0, 15, 12, 9, 22].\n",
      "The first 10 values in y are [5, 13, 13, 1, 0, 15, 12, 9, 22, 9].\n"
     ]
    }
   ],
   "source": [
    "X = [] # First letters in bigrams\n",
    "y = [] # X's bigram pairs.\n",
    "\n",
    "for name in names:\n",
    "    for char1, char2 in zip(\".\" + name, name + \".\"):\n",
    "        # Convert chars to ints so we can later add to tensors.\n",
    "        X.append(stoi[char1])\n",
    "        y.append(stoi[char2])\n",
    "\n",
    "\n",
    "print(f\"The first 10 values in X are {X[:10]}.\")\n",
    "print(f\"The first 10 values in y are {y[:10]}.\")\n",
    "# Convert list to a tensor.\n",
    "X = torch.tensor(X)\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a7ed58-185f-445d-878b-20cad9d1dd62",
   "metadata": {},
   "source": [
    "<!-- Now we are ready to get started on our neural network. Before we can make input our Xs, however,  -->\n",
    "<!-- Before we can input this data into our  -->\n",
    "<!-- Now we are ready to get started on our neural network.  -->\n",
    "<!-- For this we will use a very basic network with only a single layer of neurons. -->\n",
    "Now our data is almost ready for use in our neural network. But we don't actually want our network's weights to be multiplied by the integers in X. Instead, we'll use one-hot encoding so all possible letters are treated equally: as an array of 0s and a single 1 in the index signifying our letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f173b44-4bb1-485e-8aee-3d49fd3945e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# One-hot encode X to turn each letter into an array of length 27 (one index for each possible letter including \".\".)\n",
    "Xenc = F.one_hot(X, num_classes = 27)\n",
    "# The neural net needs to take in floats so it can be multiplied by float weights.\n",
    "Xenc = Xenc.float()\n",
    "print(Xenc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f2ce8a-281e-4ba2-b1c8-1a2ba14202d8",
   "metadata": {},
   "source": [
    "<!-- Now let's make our neural net. For this we'll use a very simple neural network with only a single layer of neurons.  -->\n",
    "<!-- Now we're ready to make our neural net. We'll give each  -->\n",
    "\n",
    "First, let's explore what this neural net would look like with only a single neuron. Even though it is only one neuron, it will need to have 27 weights, since all our inputs are size 27 and need a weight for each element within them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aada1ff3-53ff-4a9c-8f28-3721315ec2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6458],\n",
       "        [-0.8586],\n",
       "        [-0.0919],\n",
       "        [ 1.4253],\n",
       "        [-1.0649],\n",
       "        [-1.5436],\n",
       "        [-3.0012],\n",
       "        [-0.9322],\n",
       "        [-1.7632],\n",
       "        [ 1.1040],\n",
       "        [-0.1884],\n",
       "        [ 0.5235],\n",
       "        [-0.2576],\n",
       "        [-0.6203],\n",
       "        [-0.9888],\n",
       "        [-0.7899],\n",
       "        [ 0.2243],\n",
       "        [-0.1580],\n",
       "        [-0.2241],\n",
       "        [ 1.4914],\n",
       "        [ 0.5052],\n",
       "        [ 0.2037],\n",
       "        [ 0.2657],\n",
       "        [-0.4969],\n",
       "        [-1.1664],\n",
       "        [ 0.9730],\n",
       "        [-0.8801]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the weights contain random numbers from a normal distribution.\n",
    "W = torch.randn((27, 1))    # Only one neuron so far, with one weight for each element in our input.\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8655ed2f-f9c9-4c81-9e7c-da11a1c82503",
   "metadata": {},
   "source": [
    "The first letter in X is \".\", which is encoded as a 1 followed by 26 zeros. Thus, multiplying it with W would apply the dot product, selecting the first row of W (since all the other weights would get multiplied by 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eff33f14-2848-4abb-b986-8de7a5f697b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.6458])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We expect the outcome here to be equal to W[0].\n",
    "Xenc[0] @ W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9888804f-435a-474b-92d6-897a4553b3ba",
   "metadata": {},
   "source": [
    "<!-- This only gives us a single value per input. We want  -->\n",
    "<!-- This value can be used to calculate the probability  -->\n",
    "A single neuron isn't enough here, since we need a neuron for each possible bigram pair so we can use it can calculate their probabilities. We can fix this by using a full layer of 27 neurons, one for each possible label.\n",
    "\n",
    "<!-- Instead, let's make this layer contain a neuron for each possible label -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97b3146d-410a-4178-809e-64fd8bbb98a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0763,  1.7094, -0.0799,  0.8039,  0.9020, -0.2322,  0.5570,  1.0281,\n",
       "        -1.0990, -0.0564, -0.6055,  0.2821,  0.1357, -0.4425, -1.2244, -1.1211,\n",
       "         0.3559,  1.1136,  0.9501,  0.2514, -0.4165,  0.2189, -0.0048, -0.5803,\n",
       "         0.0824, -1.3561,  0.5279])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New weights should have 27 columns; one for each neuron.\n",
    "W = torch.randn((27, 27))\n",
    "# Let's take a peek at the first row of weights.\n",
    "W[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24b6683-3fef-4c91-9c29-ad00cd258324",
   "metadata": {},
   "source": [
    "<!-- Now multiplying it by Xenc should result in a vector of length 27, as it will -->\n",
    "Now the code `Xenc[0] @ W` will instead select the first weight of each neuron and result in a vector of all those selected weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1255b0d0-e2ed-4310-8f76-07a4f691236f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0763,  1.7094, -0.0799,  0.8039,  0.9020, -0.2322,  0.5570,  1.0281,\n",
       "        -1.0990, -0.0564, -0.6055,  0.2821,  0.1357, -0.4425, -1.2244, -1.1211,\n",
       "         0.3559,  1.1136,  0.9501,  0.2514, -0.4165,  0.2189, -0.0048, -0.5803,\n",
       "         0.0824, -1.3561,  0.5279])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This should result in the same values as shown above.\n",
    "Xenc[0] @ W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56317ae-556e-43da-bda6-7230282266c9",
   "metadata": {},
   "source": [
    "Likewise, each other element of Xenc selects the row of W corresponding to the position of the 1 in its one-hot encoding. For example, the input letter \"a\" will select row 1, the input b will select row 2, and so on. Since each letter corresponds to a unique row in the matrix, we want that row to represent the probabilities of each next letter following the input letter that selected it.\n",
    "\n",
    "Right now, however, our weights don't look very much like probabilities. They are negative and don't sum up to 1 (100%). We can fit the issue of negatives by exponentiating the values, making  all the digits positive while keeping values that were negative smaller than values that were positive. Then we can turn this into a probability by summing up each row and setting the weights for that row equal to their fraction of that sum. This process is called the softmax function.\n",
    "\n",
    "<!-- Then if we sum up the rows, we can set each weight equal to its fraction of that sum,  -->\n",
    "\n",
    "<!-- Then making them sum to 1 is a simple matter of setting them all equal to their  -->\n",
    "<!-- To fix this, we will treat the output of our model as logits and apply the softmax function to it. This should result in each row having a sum of 1 (100%). -->\n",
    "\n",
    "<!-- Thus, we'd like the rows to tell us the probability of each letter appearing after the letter that selects that row. -->\n",
    "<!-- Thus, each row can be thought of as the  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9acc25d7-38a8-45a6-8208-5a6b62d83f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0284, 0.1456, 0.0243,  ..., 0.0286, 0.0068, 0.0447],\n",
       "        [0.0045, 0.0095, 0.0589,  ..., 0.0055, 0.1205, 0.0188],\n",
       "        [0.0472, 0.0222, 0.0123,  ..., 0.0049, 0.0612, 0.0220],\n",
       "        ...,\n",
       "        [0.0104, 0.0089, 0.0651,  ..., 0.0099, 0.0077, 0.0136],\n",
       "        [0.0146, 0.0598, 0.0418,  ..., 0.0349, 0.0080, 0.0565],\n",
       "        [0.0196, 0.0242, 0.0164,  ..., 0.0034, 0.0034, 0.0069]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = Xenc @ W\n",
    "# After exponentiating, these values can be trained to find the count of each second letter in the bigram.\n",
    "counts = logits.exp()\n",
    "# Sum the column vectors to calculate the sum of each row, then divide each element in the row by the result to get their probabilities.\n",
    "probs = counts / counts.sum(dim=1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1127114c-d8e0-477c-9e68-06be1150f04f",
   "metadata": {},
   "source": [
    "Now that our model is set up, we're ready to train it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
